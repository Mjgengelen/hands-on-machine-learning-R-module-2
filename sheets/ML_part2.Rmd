---
title: "Hands-on Machine Learning with R - Module 2"
author: "Katrien Antonio & Roel Henckaerts"
date: '[hands-on-machine-learning-R-module-2](https://github.com/katrienantonio/hands-on-machine-learning-R-module-2) | January 14 & 21, 2021'
output:
  xaringan::moon_reader:
    css:
    - default
    - css/metropolis.css
    - css/metropolis-fonts.css
    - css/my-css.css
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      highlightLines: yes
      countIncrementalSlides: no
      highlightSpans: yes
  html_document:
    df_print: paged
subtitle: Hands-on webinar  <html><div style='float:left'></div><hr
  align='center' color='#116E8A' size=1px width=97%></html>
graphics: yes
editor_options:
  chunk_output_type: console
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,shapes, snakes, arrows}
- \usepackage{amsfonts}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{color}
- \usepackage{graphicx}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# options(knitr.table.format = "html")
library(tidyverse)
library(fontawesome) # from github: https://github.com/rstudio/fontawesome
library(DiagrammeR)
library(emo) # from github: https://github.com/hadley/emo
library(gt) # from github: https://github.com/rstudio/gt
library(countdown) # from github: https://github.com/gadenbuie/countdown 
library(here)

library(gganimate)
library(transformr)

ggplot2::theme_set(theme_bw())
plot_pred_reg <- function(dt, preds){
  dt %>% mutate(pred = preds) %>% ggplot(aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = m), colour = 'darkgreen', size = 1.5) +
  geom_line(aes(y = pred), colour = 'darkred', size = 1.5)
}
plot_pred_class <- function(dt, preds){
  dt %>% mutate(pred = preds) %>% ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = pred))
}
```

```{r setup_greenwell, include=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE, 
        crayon.enabled = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# colors - I copied most of these from # https://github.com/edrubin/EC524W20
dark2 <- RColorBrewer::brewer.pal(8, name = "Dark2")
KULbg <- "#116E8A"
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

class: inverse, center, middle
name: prologue

# Prologue

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: introduction

# Introduction

### Course

`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/hands-on-machine-learning-R-module-2

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

`r fa(name = "link", fill = KULbg)` [https://katrienantonio.github.io/](https://katrienantonio.github.io/) & [https://henckr.github.io/](https://henckr.github.io/)

`r fa(name = "paper-plane", fill = KULbg)` [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) & [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

`r fa('graduation-cap', fill = KULbg)` (Katrien) Professor in insurance data science

`r fa('graduation-cap', fill = KULbg)` (Roel) PhD student in insurance data science

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  ```{r eval=TRUE}
  version$version.string
  ```

☑ Do you have a fairly recent version of RStudio? 
  ```{r eval=FALSE}
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```

☑ Have you installed the R packages listed in the software requirements? 

or

☑ Have you created an account on RStudio Cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this module .font140[`r fa(name = "fas fa-rocket", fill = KULbg)`]

--

* develop foundations of working with .KULbginline[regression and decision trees]

--

* step from simple trees to ensembles of trees, with .KULbginline[bagging] and .KULbginline[boosting]

--

* focus on the use of these ML methods for the .KULbginline[analysis of frequency + severity data] 

--

* discuss and construct some useful .KULbginline[interpretation tools], e.g. variable importance plots, partial dependence plots.

---

# Module 2's Outline

.pull-left[

* [Prologue](#prologue)

* [Decision tree](#tree)

  - what is tree-based machine learning?
  - tree basics: structure, terminology, growing process
  - using {rpart}
  - pruning via cross-validation
  - examples on regression and classification
  - modelling claim frequency and severity data with trees
  
* [Interpretation tools](#interpret)

 - feature importance
 - partial dependence plot
 - the {vip} and {pdp} packages

]

.pull-right[

* [Bagging](#bag) 

 - from a single tree to Bootstrap Aggregating
 - out-of-bag error

* [Random forest](#rf)

 - from bagging to random forests
 - tuning

* [Gradient boosting](#gb)

 - (stochastic) gradient boosting with trees
 - training process and tuning parameters
 - using {gbm}
 - modelling claim frequencies and severities
 - using {xgboost}

]


---

name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

<img src = "img/AI_ML_DL.jpg" height = "350px" />

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---

# Background reading

.left-column[

<br>

<img src = "img/naaj.png" height = "350px" />

]

.right-column[

Henckaerts et al. (2020) paper on [Boosting insights in insurance tariff plans with tree-based machine learning methods](https://katrienantonio.github.io/publication/2020-boosting/)

- full algorithmic details of regression trees, bagging, random forests and gradient boosting machines
- with focus on claim frequency and severity modelling
- including interpretation tools (VIP, PDP, ICE, H-statistic)
- model comparison (GLMs, GAMs, trees, RFs, GBMs) 
- managerial tools (e.g. loss ratio, discrimination power).

The paper comes with two notebooks, see [examples tree-based paper](https://github.com/henckr/treeML) and [severity modelling](https://github.com/henckr/sevtree).

The paper comes with an R package for fitting random forests on insurance data, see [distRforest](https://github.com/henckr/distRforest).

]

---

# What is tree-based machine learning?

.KULbginline[Machine learning (ML)] according to [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning):

> *"Machine learning algorithms build a .hi-pink[mathematical model] based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to perform the task."*

> This definition goes all the way back to [Arthur Samuel](https://en.wikipedia.org/wiki/Arthur_Samuel), who coined the term "machine learning" in 1959.

--

.KULbginline[Tree-based ML] makes use of a .KULbginline[tree] as building block for the mathematical model.

```{r tree_based, out.width='70%', echo=FALSE}
knitr::include_graphics('img/tree_based.png')
```

--

So, a natural question to start from is: what is a .KULbginline[tree]?

---



class: inverse, center, middle
name: tree-basic

# Tree basics

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---


class: clear
background-image: url(img/decision_tree.jpg)
background-size: contain


---

# Tree structure and terminology

The top of the tree contains all available training observations: the .hi-pink[root node].

--

We .KULbginline[partition] the data into homogeneous non-overlapping subgroups: the .hi-pink[nodes]

--

We create subgroups via .KULbginline[simple yes-no questions].

--

A tree then predicts the output in a .hi-pink[leaf node] as follows:

  + average of the response for regression
  + majority voting for classification


---

# Tree structure and terminology

The top of the tree contains all available training observations: the .hi-pink[root node].


We .KULbginline[partition] the data into homogeneous non-overlapping subgroups: the .hi-pink[nodes].

```{r out.width= "50%", out.extra='style="float:right; padding:10px"', echo=FALSE}
knitr::include_graphics("img/tree_example.jpg")
```

We create subgroups via .KULbginline[simple yes-no questions].

A tree then predicts the output in a .hi-pink[leaf node] as follows:

  + average of the response for regression
  + majority voting for classification
  
Different types of nodes:

```{r out.width= "27%", echo=FALSE}
knitr::include_graphics("img/tree_legend.jpg")
```


---

# Tree growing process


A golden standard is the .KULbginline[C]lassification .KULbginline[A]nd .KULbginline[R]egression .KULbginline[T]ree algorithm: .KULbginline[CART] (Breiman et al., 1984).

--

CART uses .KULbginline[binary recursive partitioning] to split the data in subgroups.

--

In each node, we search for the best feature to partition the data into two regions: R<sub>1</sub> and R<sub>2</sub> (hence, .KULbginline[binary]).

--

.font140[.KULbginline[Take-away]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; - &nbsp; what is .KULbginline[best?] 

Minimize the .KULbginline[overall loss] between observed responses and leaf node prediction
  + overall loss = loss in region R<sub>1</sub> + loss in region R<sub>2</sub>
  + for regression: mean squared or absolute error, deviance,...
  + for classification: cross-entropy, Gini index,...

--

After splitting the data, this process is repeated for region R<sub>1</sub> and R<sub>2</sub> separately (hence, .KULbginline[recursive]).

--

Repeat until .KULbginline[stopping criterion] is satisfied, e.g., maximum depth of a tree or minimum loss improvement.


---

# Using {rpart}

```{r, eval=FALSE}
rpart(formula, data, method,
      control = rpart.control(cp, maxdepth, minsplit, minbucket))
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .font200[`r fa(name = 'bullhorn', fill = KULbg)`] &nbsp;&nbsp; no need to include the interactions!

* `data`: the observation data containing the response and features

* `method`: a string specifying which .hi-pink[loss function] to use
  + "anova" for regression (SSE as loss)
  + "class" for classification (Gini as loss)
  + "poisson" for Poisson regression (Poisson deviance as loss, see more later)
  
* `cp`: complexity parameter specifying the proportion by which the overall error should improve for a split to be attempted
  
* `maxdepth`: the maximum depth of the tree

* `minsplit`: minimum number of observations in a node for a split to be attempted

* `minbucket`: minimum number of observations in a leaf node.

---

class: inverse, center, middle
name: toy-regr

# Toy example of a regression tree

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

# Simulated data

.pull-left[
```{r}
library(tidyverse)
set.seed(54321) # reproducibility
dfr <- tibble::tibble(
  x = seq(0, 2 * pi, length.out = 500),
  m = 2 * sin(x),
  y = m + rnorm(length(x), sd = 1)
  )
```
```{r, echo=FALSE}
dfr[1:10,] %>% as.data.frame() %>% print()
```
]

.pull-right[
```{r, echo = FALSE}
ggplot(dfr, aes(x = x)) + geom_point(aes(y = y), alpha = 0.3) + geom_line(aes(y = m), colour = 'darkgreen', size = 1.5)
```
]

---

# Decision stump - a tree with only one split

.pull-left[
```{r}
library(rpart)
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova', #<<
             control = rpart.control(
               maxdepth = 1 #<<
               )
             )
print(fit)
```
```{r, eval=FALSE}
library(rpart.plot) # for nice plots
rpart.plot(fit, digits = 4, cex = 2) #<<
```
]

.pull-right[
```{r,echo=FALSE}
library(rpart.plot)
rpart.plot(fit, digits = 4, cex = 2)
```
]

---
# Decision stump - a tree with only one split

.pull-left[
```{r}
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova', #<<
             control = rpart.control(
               maxdepth = 1 #<<
               )
             )
print(fit)
```
```{r,eval=FALSE}
# Get predictions via the predict function
pred <- predict(fit, dfr) #<<
```
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit, dfr))
```
]

---

# Adding splits

.pull-left[
```{r}
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 2 #<<
               )
             )
print(fit)
```
]

.pull-right[
```{r, echo=FALSE}
rpart.plot(fit, digits = 4, cex = 1.5)
```
]

---

# Adding splits (cont.)

.pull-left[
```{r}
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 2 #<<
               )
             )
print(fit)
```
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit, dfr))
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
Let's get familiar with the structure of a decision tree. <br> <br>

.hi-pink[Q]: choose one of the trees from the previously discussed examples and pick a leaf node, but keep it simple for now.

1. Replicate the .KULbginline[predictions] for that leaf node, based on the split(s) and the training data.

1. Replicate the .KULbginline[deviance] measure for that leaf node, based on the split(s), the training data and your predictions from Q1. 

Hint: the deviance used in an anova {rpart} tree is the .hi-pink[Sum of Squared Errors (SSE)]:

$$\begin{eqnarray*}
\textrm{SSE} = \sum_{i=1}^n (\color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i))^2,
\end{eqnarray*}$$

]

---

class: clear

.pull-left[
Take for example the tree with depth two: 
```{r}
print(fit)
```
Let's predict the values for leaf node 6.
]

.pull-right[
.hi-pink[Q.1]: calculate the prediction
```{r}
# Subset observations in node 6
obs <- dfr %>% dplyr::filter(x < 0.535141)

# Predict
pred <- obs$y %>%  mean
pred
```

.hi-pink[Q.2]: calculate the deviance
```{r}
# Deviance
dev <- (obs$y - pred)^2 %>% sum
dev
```
]


---

# A very deep tree

.pull-left[
```{r}
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 10, #<<
               minbucket = 5, #<<
               cp = 0 #<<
               )
             )
```

.font140[.KULbginline[Take-away]] .font200[`r fa(name = 'bullhorn', fill = KULbg)`] &nbsp; - &nbsp; understanding the `cp` parameter:

  - unitless in {rpart} (different from original CART)
  - `cp = 1` returns a .hi-pink[root node], without splits
  - `cp = 0` returns the .hi-pink[deepest tree possible], allowed by the other stopping criteria.
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, predict(fit, dfr))
```
]

---

# A very deep tree (cont.)

.pull-left[
```{r}
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 10, #<<
               minbucket = 5, #<<
               cp = 0 #<<
               )
             )
```

<br> 

What is your opinion on the tree shown on the right?

]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, predict(fit, dfr))
```
]

---

class: inverse, center, middle
name: prune-tree

# Pruning via cross-validation in {rpart}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

#  How deep should a tree be?

.pull-left[

The .KULbginline[bias-variance trade off]:
  - a .hi-pink[shallow] tree will underfit: 
  <br>
  bias .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`] and variance .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] <br>
  - a .hi-pink[deep] tree will overfit: 
  <br>
  bias .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] and variance .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`] <br>
  - find right .KULbginline[balance] between bias and variance!

Typical approach to get the right fit:
  - fit an overly complex .KULbginline[deep tree]
  - .KULbginline[prune] the tree to find the .KULbginline[optimal subtree].
  
How to .KULbginline[prune?]
]


---

#  How deep should a tree be?

.pull-left[

The .KULbginline[bias-variance trade off]:
  - a .hi-pink[shallow] tree will underfit: 
  <br>
  bias .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`] and variance .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] <br>
  - a .hi-pink[deep] tree will overfit: 
  <br>
  bias .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] and variance .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`] <br>
  - find right .KULbginline[balance] between bias and variance!

Typical approach to get the right fit:
  - fit an overly complex .KULbginline[deep tree]
  - .KULbginline[prune] the tree to find the .KULbginline[optimal subtree].
  
How to .KULbginline[prune?]

]


.pull-right[



Minimize a .KULbginline[penalized loss function] during training:
  $$\min\{f_{\textrm{loss}} + \alpha |T|\}$$
  + loss function $f_{\textrm{loss}}$
  + complexity parameter $\alpha$
  + number of leaf nodes $|T|$.

A shallow tree results when $\alpha$ is large and a deep tree when $\alpha$ is small.
  
Perform .hi-pink[cross-validation] on the parameter $\alpha$
  + `cp` is the complexity parameter in {rpart}.
  
Same idea as the lasso from {glmnet} in .KULbginline[Module 1].
]

---

# Pruning via cross-validation

.pull-left[
```{r}
set.seed(87654) # reproducibility
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 10,
               minsplit = 20,
               minbucket = 10,
               cp = 0, #<<
               xval = 5 #<<
               )
             )
```
```{r, eval=FALSE}
# Plot the cross-validation results
plotcp(fit) #<<
```
]

.pull-right[
```{r,echo=FALSE}
plotcp(fit)
```
]

---
 
# Pruning via cross-validation

.pull-left[
```{r}
set.seed(87654) # reproducibility
fit <- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 10,
               minsplit = 20,
               minbucket = 10,
               cp = 0, #<<
               xval = 5 #<<
               )
             )
# Get xval results via 'cptable' attribute
cpt <- fit$cptable #<<
```
```{r,eval=FALSE}
print(cpt[1:20,]) #<<
# Which cp value do we choose?
min_xerr <- which.min(cpt[,'xerror'])
se_rule <- min(which(cpt[, 'xerror'] < 
  (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
```
]

.pull-right[
```{r,echo=FALSE}
print(cpt[1:20,], digits = 6)
```
]


---

# Minimal CV error or 1 SE rule

.pull-left[
```{r, echo = FALSE}
min_xerr <- which.min(cpt[,'xerror'])
```
```{r}
fit_1 <- prune(fit, cp = cpt[min_xerr, 'CP']) #<<
```
```{r, out.width='85%', echo = FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit_1, dfr))
```
]

.pull-right[
```{r, echo=FALSE}
se_rule <- min(which(cpt[, 'xerror'] < (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
```
```{r}
fit_2 <- prune(fit, cp = cpt[se_rule, 'CP']) #<<
```
```{r, out.width='85%', echo = FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit_2, dfr))
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[

.hi-pink[Q]: Trees are often associated with .KULbginline[high variance], meaning that the resulting model can be very sensitive to the input data. Let's explore this statement!

1. Generate a second data set `dfr2` with a different seed.

1. Fit an optimal tree to this data following the pruning strategy. 

1. Can you spot substantial differences with the trees from before?

<br> 

.hi-pink[Q.1]: a brand new data set

```{r}
# Generate the data
set.seed(83625493)
dfr2 <- tibble(
  x = seq(0, 2 * pi, length.out = 500),
  m = 2 * sin(x),
  y = m + rnorm(length(x), sd = 1)
  )
```
]

---

class: clear

```{r, include=FALSE}
set.seed(87654)
fit <- rpart(formula = y ~ x,
             data = dfr2,
             method = 'anova',
             control = rpart.control(
               maxdepth = 10,
               minsplit = 20,
               minbucket = 10,
               cp = 0, #<<
               xval = 5 #<<
               )
             )
# Get xval results via 'cptable' attribute
cpt <- fit$cptable #<<
```

.pull-left[
.hi-pink[Q.2a]: optimal tree with .KULbginline[min CV error]
```{r, echo = FALSE}
min_xerr <- which.min(cpt[,'xerror'])
fit_1 <- prune(fit, cp = cpt[min_xerr, 'CP'])
```
```{r, out.width='85%', echo = FALSE}
plot_pred_reg(dt = dfr2, preds = predict(fit_1, dfr2))
```
]

.pull-right[
.hi-pink[Q.2b]: optimal tree with .KULbginline[one SE rule]
```{r, echo=FALSE}
se_rule <- min(which(cpt[, 'xerror'] < (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
fit_2 <- prune(fit, cp = cpt[se_rule, 'CP'])
```
```{r, out.width='85%', echo = FALSE}
plot_pred_reg(dt = dfr2, preds = predict(fit_2, dfr2))
```
]

<br>

.hi-pink[Q.3]: trees look .KULbginline[rather different] compared to those from before, even though they try to approximate the same function.


---

class: inverse, center, middle
name: toy-class

# Toy example of a classification tree

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Simulated data

.pull-left[
```{r}
set.seed(54321) # reproducibility
dfc <- tibble::tibble(
  x1 = rep(seq(0.1,10,by = 0.1), times = 100),
  x2 = rep(seq(0.1,10,by = 0.1), each = 100),
  y = as.factor(
    pmin(1,
         pmax(0,
              round(
      1*(x1+2*x2<8) + 1*(3*x1+x2>30) + 
        rnorm(10000,sd = 0.5))
              )
        )
    )
)
```
]

.pull-right[
```{r, echo = FALSE}
ggplot(dfc, aes(x = x1, y = x2)) + geom_point(aes(color = y))
```
]

---

# Fitting a simple tree 

.pull-left[
```{r}
fit <- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class', #<<
             control = rpart.control(
               maxdepth = 2 #<<
               )
             )
print(fit)
```
]

.pull-right[
```{r, out.width='95%', echo=FALSE}
rpart.plot(fit, digits = 4, cex = 1.5)
```
]


---

# Fitting a simple tree (cont.)

.pull-left[
```{r}
fit <- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class', #<<
             control = rpart.control(
               maxdepth = 2 #<<
               )
             )
print(fit)
```
]

.pull-right[
```{r, echo=FALSE}
plot_pred_class(dt = dfc, preds = predict(fit, dfc, type = 'class'))
```
]

---

# What about an overly complex tree?

.pull-left[
```{r}
fit <- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 10, #<<
               minbucket = 5, #<<
               cp = 0 #<<
               )
             )
```
]

.pull-right[
```{r, echo=FALSE}
plot_pred_class(dt = dfc, preds = predict(fit, dfc, type = 'class'))
```
]

---

# What about an overly complex tree?

.pull-left[
```{r}
fit <- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 10, #<<
               minbucket = 5, #<<
               cp = 0 #<<
               )
             )
```
<br> 
Clearly .KULbginline[overfitting]!
]

.pull-right[
```{r, echo=FALSE}
plot_pred_class(dt = dfc, preds = predict(fit, dfc, type = 'class'))
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
Let's find a satisfying fit for this classification example. <br>
.hi-pink[Q]: perform .KULbginline[cross-validation] on `cp` to find the .KULbginline[optimal pruned subtree].

1. Set `xval = 5` in `rpart.control()` (do not forget to set a .hi-pink[seed] beforehand).

1. Graphically inspect the xval results via `plotcp()`.

1. Extract the xval results via `$cptable`.

1. Apply the min xerror and/or the one SE rule to find the .hi-pink[optimal] `cp`.

1. Show the resulting classifier graphically.


]

---

class: clear

.pull-left[
.hi-pink[Q.1]: fit a complex tree and perform cross-validation
```{r}
set.seed(87654) # reproducibility
fit <- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
               cp = 0, #<<
               xval = 5 #<<
               )
             )
```
]

.pull-right[
.hi-pink[Q.2]: inspect the xval results graphically
```{r}
plotcp(fit)
```
]

---

class: clear

.pull-left[
.hi-pink[Q.3]: extract the xval results in a table
```{r}
# Get xval results via 'cptable' attribute
cpt <- fit$cptable
```
.hi-pink[Q.4]: optimal `cp` via min CV error or one SE rule
```{r}
# Which cp value do we choose?
min_xerr <- which.min(cpt[,'xerror'])

se_rule <- min(which(cpt[, 'xerror'] < 
  (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
```
```{r}
unname(min_xerr)
```
```{r}
se_rule
```


]

.pull-right[
```{r}
print(cpt[16:35,], digits = 6)
```
]

---

class: clear

.pull-left[
.hi-pink[Q.5a]: optimal subtree via .KULbginline[min CV error]
```{r}
fit_1 <- prune(fit, cp = cpt[min_xerr, 'CP'])
```
```{r, out.width='85%', echo = FALSE}
plot_pred_class(dt = dfc, preds = predict(fit_1, dfc, type = 'class'))
```
]

.pull-right[
.hi-pink[Q.5b]: optimal subtree via .KULbginline[one SE rule]
```{r}
fit_2 <- prune(fit, cp = cpt[se_rule, 'CP'])
```
```{r, out.width='85%', echo = FALSE}
plot_pred_class(dt = dfc, preds = predict(fit_2, dfc, type = 'class'))
```
]

---

class: inverse, center, middle
name: freq-sev-tree

# Claim frequency and severity modeling with {rpart}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---


# Claim frequency prediction on the MTPL data

Recall the MTPL data set introduced in Module 1.

The .KULbginline[Poisson GLM] is a classic approach for modelling .KULbginline[claim frequency] data.

How to deal with claim counts in a decision tree?

Use the .KULbginline[Poisson deviance] as .hi-pink[loss function]:
 
$$\begin{eqnarray*}
D^{\textrm{Poi}} = \frac{2}{n} \sum_{i=1}^{n} \color{#FFA500}{y}_i \cdot \ln \frac{\color{#FFA500}{y}_i}{\textrm{expo}_i \cdot \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)} - \{\color{#FFA500}{y}_i - \textrm{expo}_i \cdot \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)\},
\end{eqnarray*}$$

with $\textrm{expo}$ the exposure measure.


--

Here we go: 

```{r}
# Read the MTPL data
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) 
mtpl <- read.table('../data/PC_data.txt',
                   header = TRUE, stringsAsFactors = TRUE) %>% 
  as_tibble() %>% rename_all(tolower) %>% rename(expo = exp)
```

---

# Fitting a simple tree to the MTPL data

.pull-left[
```{r}
fit <- rpart(formula = 
               cbind(expo,nclaims) ~ #<<
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl, #<<
             method = 'poisson', #<<
             control = rpart.control(
               maxdepth = 3, #<<
               cp = 0)
             )
```
```{r,eval=FALSE}
print(fit)
```


.font140[.KULbginline[Take-away]] .font200[`r fa(name = 'bullhorn', fill = KULbg)`]  &nbsp; - &nbsp; .KULbginline[Poisson tree] in {rpart}: 
  - Poisson deviance via `method = 'poisson'` <br>
  - response as two-column matrix: `cbind(expo,y)`.
]

.pull-right[
```{r,echo=FALSE}
print(fit)
```
]

---

# Fitting a simple tree to the MTPL data

.pull-left[
```{r, eval=FALSE}
fit <- rpart(formula = 
               cbind(expo,nclaims) ~ #<<
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl, #<<
             method = 'poisson', #<<
             control = rpart.control(
               maxdepth = 3, #<<
               cp = 0)
             )
```
```{r,eval=FALSE}
print(fit)
```
<br>
Easier way to .KULbginline[understand] this tree? <br>
Try `rpart.plot` from the package .KULbginline[{rpart.plot}]

]

.pull-right[
```{r,echo=FALSE}
print(fit)
```
]

---

# Fitting a simple tree to the MTPL data

.center[
```{r,fig.width=15 ,echo=FALSE}
rpart.plot(fit, cex = 1.5)
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]

.right-column[
Verify whether the .KULbginline[prediction] in a leaf node is .hi-pink[what you would expect]. <br>

.hi-pink[Q]: take the leftmost node as an example: `bm < 2` and `ageph >= 56`.

1. Subset the data accordingly.

1. Calculate the expected claim frequency as `sum(nclaims)/sum(expo)`.

1. Compare with the {rpart} prediction of 0.08899811.
]

---

class:clear

.hi-pink[Q.1-Q.2]: subset the data and calculate the claim frequency
```{r, eval=FALSE}
mtpl %>% 
  dplyr::filter(bm < 2,
                ageph >= 56) %>% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo))
```
```{r, echo=FALSE}
mtpl %>% 
  dplyr::filter(bm < 2,
                ageph >= 56) %>% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo)) %>% as.data.frame()
```

.hi-pink[Q.3]: the prediction and our DIY calculation .KULbginline[do not match]! <br>

Is this due to a rounding error? <br>
Or is there something spooky .font200[`r fa(name = 'ghost', fill = KULbg)`] going on?

---

# Unraveling the mystery of {rpart}

.pull-left[

.KULbginline[Conceptually]: no events in a leaf node lead to division by zero in the deviance!

Solution: assume .KULbginline[Gamma prior] on the mean of the Poisson in the leaf nodes: 
  + set $\mu = \sum y_i / \sum \textrm{expo}_i$
  + use coefficient of variation $k = \sigma / \mu$ as .KULbginline[user input]
  + $k = 0$ extreme .KULbginline[pessimism] (all leaf nodes equal)
  + $k = \infty$ extreme .KULbginline[optimism] (let the data speak)
  + default in {rpart}: $k=1$.
  
The resulting leaf node prediction: 

$$\frac{\alpha + \sum Y_i}{\beta + \sum \text{expo}_i}, \,\,\,\,\,\,\, \alpha = 1/k^2, \,\,\,\,\,\,\, \beta=\alpha / \mu.$$
]

.pull-right[
```{r,eval=FALSE}
k <- 1

alpha <- 1/k^2

mu <- mtpl %>% 
  with(sum(nclaims)/sum(expo))

beta <- alpha/mu

mtpl %>% 
  dplyr::filter(bm < 2, ageph >= 56) %>% 
  dplyr::summarise(prediction = 
          (alpha + sum(nclaims))/(beta + sum(expo))) #<<
```
```{r,echo=FALSE}
k <- 1

alpha <- 1/k^2

mu <- mtpl %>% 
  with(sum(nclaims)/sum(expo))

beta <- alpha/mu

mtpl %>% 
  dplyr::filter(bm < 2, ageph >= 56) %>% 
  dplyr::summarise(prediction = 
          (alpha + sum(nclaims))/(beta + sum(expo))) %>% as.data.frame()
```

More details in Section 8.2 of the [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) on Poisson regression. 


]

---

# Coefficient of variation very low

.pull-left[
```{r}
fit <- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               cp = 0),
             parms = list(shrink = 10^-5) #<<
             )
```
<br>
Notice that .KULbginline[all] leaf nodes predict the .KULbginline[same value].
]

.pull-right[
```{r,echo=FALSE}
print(fit)
```
]

---

# Coefficient of variation very high

.pull-left[
```{r}
fit <- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               cp = 0),
             parms = list(shrink = 10^5) #<<
             )
```
```{r,eval=FALSE}
# Remember this number?
mtpl %>% 
  dplyr::filter(bm < 2, ageph >= 56) %>% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo))
```
```{r,echo=FALSE}
# Remember this number?
mtpl %>% 
  dplyr::filter(bm < 2, ageph >= 56) %>% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo)) %>% as.data.frame()
```

]

.pull-right[
```{r,echo=FALSE}
print(fit)
```
]

---


name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]

.right-column[
.hi-pink[Q]: Follow the .KULbginline[pruning strategy] to develop a proper frequency tree model for the MTPL data.

1. Start from an overly complex tree. Do not forget your favorite random .hi-pink[seed] upfront!

1. Inspect the cross-validation results.

1. Choose the `cp` value minimizing `xerror` for .KULbginline[pruning].

1. Visualize the pruned tree with `rpart.plot`.
]

---

class:clear

.pull-left[
.hi-pink[Q.1]: fit an overly complex tree
```{r}
set.seed(9753) # reproducibilty
fit <- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 2000, #<<
               minbucket = 1000, #<<
               cp = 0,
               xval = 5 #<<
               )
             )
```
]

.pull-right[
.hi-pink[Q.2]: inspect the cross-validation results
```{r}
plotcp(fit)
```
]

---

class: clear

.hi-pink[Q.3]: choose the `cp` value that minimizes `xerror` for .KULbginline[pruning]
```{r}
# Get the cross-validation results
cpt <- fit$cptable

# Look for the minimal xerror
min_xerr <- which.min(cpt[,'xerror'])
cpt[min_xerr,]

# Prune the tree
fit_srt <- prune(fit,
                 cp = cpt[min_xerr, 'CP'])
```

---

class: clear

.hi-pink[Q.4]: try to understand how the final model looks like. Can you make sense of it?
.center[
```{r,fig.width=16, echo=FALSE}
rpart.plot(fit_srt, type = 0, extra = 0, cex = 1.1)
```
]

---

class: inverse, center, middle
name: interpret

# Interpretation tools

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Interpreting a tree model

.pull-left[

Interpretability depends on the .KULbginline[size of the tree]
  + is easy with a .hi-pink[shallow] tree but hard with a .hi-pink[deep] tree 
  + luckily there are some .KULbginline[tools] to aid you.

.KULbginline[Feature importance]
  + identify the most .hi-pink[important] features
  + implemented in the package {vip}. 

.KULbginline[Partial dependence plot]
  + measure the .hi-pink[marginal effect] of a feature
  + implemented in the package {pdp}.

Excellent source on interpretable machine learning: [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) book by Christophe Molnar.
]

.pull-right[
```{r, out.width='40%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/logo_vip.png")
```
```{r, out.width='40%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/logo_pdp.png")
```
]

---

# Feature importance and partial dependence

.left-column[

<br>
<br>

<img src = "img/molnar.png" height = "350px" />



]

.right-column[

With .KULbginline[feature importance]: 

* sum improvements in loss function over all splits on a variable $x_{\ell}$
* important variables appear high and often in a tree.

With .KULbginline[partial dependence]: 

* univariate

$$\bar{f}_{\ell}(x_{\ell}) = \frac{1}{n} \sum_{i=1}^n f_{\text{tree}}(x_{\ell},\boldsymbol{x}^{i}_{-\ell})$$

* bivariate

$$\bar{f}_{k, \ell}(x_k, x_{\ell}) = \frac{1}{n} \sum_{i=1}^n f_{\text{tree}}(x_k, x_{\ell}, \boldsymbol{x}^{i}_{-k, \ell})$$

* marginal effects, interactions can stay hidden!

]



---

# Feature importance <img src="img/logo_vip.png" class="title-hex">

.pull-left[
```{r}
library(vip)
# Function vi gives you the data
var_imp <- vip::vi(fit_srt) #<<
```
```{r, echo=FALSE}
print(var_imp %>% as.data.frame())
```

```{r, eval=FALSE}
# Function vip makes the plot
vip::vip(fit_srt, scale = TRUE) #<<
```
]

.pull-right[
```{r, echo=FALSE}
vip::vip(fit_srt, scale = TRUE)
```
]

---

# Partial dependence plot <img src="img/logo_pdp.png" class="title-hex">

.pull-left[
```{r}
library(pdp)
# Need to define this helper function for Poisson
pred.fun <- function(object,newdata){
  mean(predict(object, newdata))
} 

# Sample 5000 observations to speed up pdp generation
set.seed(48927)
pdp_ids <- mtpl %>% nrow %>% sample(size = 5000)
```
```{r, eval = FALSE}
# partial: computes the marginal effect
# autoplot: creates the graph using ggplot2
fit_srt %>% 
  pdp::partial(pred.var = 'ageph',#<<
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %>% 
  autoplot()#<<
```
]

.pull-right[
```{r, echo = FALSE}
fit_srt %>% 
  pdp::partial(pred.var = 'ageph',
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

---

# Partial dependence plot in two dimensions <img src="img/logo_pdp.png" class="title-hex">

.pull-left[
```{r, eval = FALSE}
# partial: computes the marginal effect
# autoplot: creates the graph using ggplot2
fit_srt %>% 
  pdp::partial(pred.var = c('ageph','power'), #<<
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

.pull-right[
```{r, echo = FALSE}
fit_srt %>% 
  pdp::partial(pred.var = c('ageph','power'),
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]

.right-column[
Use partial dependence plots for .hi-pink[other features] to gain .KULbginline[understanding] of your model.
]

---

class:clear

.pull-left[
.KULbginline[Level in the bonus-malus scale] <br>
```{r, echo = FALSE}
fit_srt %>% 
  partial(pred.var = 'bm',
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

.pull-right[
.KULbginline[Type of coverage] <br>
```{r, echo = FALSE}
fit_srt %>% 
  partial(pred.var = 'coverage',
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

---

# That's a wrap on single trees!

.pull-left[

.font140[.KULbginline[Advantages]] &nbsp; &nbsp; .font200[`r fa(name = 'far fa-smile-wink', fill = KULbg)`]

* Shallow tree is easy to .KULbginline[explain] graphically.

* Closely mirror the human .KULbginline[decision-making] process.

* Handle all types of features .KULbginline[without] pre-processing.

* .KULbginline[Fast] and very scalable to big data.

* .KULbginline[Automatic] variable selection.

* Surrogate splits can handle .KULbginline[missing] data.
]

---

# That's a wrap on single trees!

.pull-left[

.font140[.KULbginline[Advantages]] &nbsp; &nbsp; .font200[`r fa(name = 'far fa-smile-wink', fill = KULbg)`]

* Shallow tree is easy to .KULbginline[explain] graphically.

* Closely mirror the human .KULbginline[decision-making] process.

* Handle all types of features .KULbginline[without] pre-processing.

* .KULbginline[Fast] and very scalable to big data.

* .KULbginline[Automatic] variable selection.

* Surrogate splits can handle .KULbginline[missing] data.
]

.pull-right[

.font140[.KULbginline[Disadvantages]] &nbsp; &nbsp; .font200[`r fa(name = 'far fa-frown', fill = KULbg)`]

* Tree uses .KULbginline[step] functions to approximate the effect.

* Greedy heuristic approach chooses .KULbginline[locally] optimal split (i.e., based on all previous splits).

* Data becomes .KULbginline[smaller] and smaller down the tree.

* All this results in .KULbginline[high variance] for a tree model...

* ... which harms .KULbginline[predictive performance].
]

---

class: inverse, center, middle
name: tree

# From a single tree to ensembles of trees

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Ensembles of trees

Remember: prediction error = bias + variance.

--

Good .KULbginline[predictive performance] requires low bias .KULbginline[AND] low variance.

--

Two popular .hi-pink[ensemble] algorithms (that can be applied to any type of model, not just trees) are:

--

* .KULbginline[bagging]:
  + low .hi-pink[bias] via detailed individual models (think: deep trees)
  + low .hi-pink[variance] via averaging of those models
  + .KULbginline[random forest] is a modification on bagging for trees to further improve the variance reduction.

* .KULbginline[boosting]:
  + low .hi-pink[variance] via simple individual models
  + low .hi-pink[bias] by incrementing the model sequentially.
  

---

# From our own experience

.center[
.KULbginline[Boosting > Random forest > Bagging > Decision tree]
]

```{r out.width="50%", echo=FALSE}
knitr::include_graphics("img/oos_freq_poiss.png")
```

Detailed discussion in our North American Actuarial Journal (2020) paper on .KULbginline[Boosting insights in insurance tariff plans with tree-based machine learning methods]. See [Henckaerts et al. (2020)](https://arxiv.org/abs/1904.10890).

---


class: inverse, center, middle
name: bag

# Introducing bagging

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

class: clear
background-image: url(img/bagging.jpg)
background-size: contain

---


# Bagging

.pull-left[

.KULbginline[Bagging] is for .KULbginline[B]ootstrap .KULbginline[AGG]regat.KULbginline[ING].

Simple idea:
  + build a lot of different .KULbginline[base learners] on bootstrapped samples of the data
  + .KULbginline[combine] their predictions.

Model .KULbginline[averaging] helps to:
  + reduce variance
  + avoid overfitting.

Bagging works best for .KULbginline[base learners] with:
  + .hi-pink[low bias] and .hi-pink[high variance]
  + for example: deep decison trees.
]

---

# Bagging

.pull-left[

.KULbginline[Bagging] is for .KULbginline[B]ootstrap .KULbginline[AGG]regat.KULbginline[ING].

Simple idea:
  + build a lot of different .KULbginline[base learners] on bootstrapped samples of the data
  + .KULbginline[combine] their predictions.

Model .KULbginline[averaging] helps to:
  + reduce variance
  + avoid overfitting.

Bagging works best for .KULbginline[base learners] with:
  + .hi-pink[low bias] and .hi-pink[high variance]
  + for example: deep decison trees.
]

.pull-right[

.KULbginline[Bagging with trees?]

* Do .KULbginline[B] times:
  + create .hi-pink[bootstrap sample] by drawing with replacement from the original data
  + fit a .hi-pink[deep tree] to the bootstrap sample.
  
* .KULbginline[Combine] the predictions of the B trees
  + .hi-pink[average] prediction for regression
  + .hi-pink[majorty] vote for classification.
  
This is implemented in the {ipred} package, using {rpart} under the hood.
]

---

# Bootstrap samples

.pull-left[
```{r}
# Set a seed for reproducibility
set.seed(45678)

# Generate the first bootstrapped sample
bsample_1 <- dfr %>% nrow %>% 
  sample(replace = TRUE) #<<
# Generate another bootstrapped sample
bsample_2 <- dfr %>% nrow %>% 
  sample(replace = TRUE) #<<

# Use the indices to sample the data
dfr_b1 <- dfr %>%
  dplyr::slice(bsample_1) #<<
dfr_b2 <- dfr %>% 
  dplyr::slice(bsample_2) #<<
```
```{r, eval=FALSE}
# Let's have a look at the sampled data
dfr_b1 %>% dplyr::arrange(x) %>% head() #<<
dfr_b2 %>% dplyr::arrange(x) %>% head() #<<
```
]

.pull-right[
Sample 1:
```{r, echo=FALSE}
dfr_b1 %>% dplyr::arrange(x) %>% as.data.frame() %>% head()
```
Sample 2:
```{r, echo=FALSE}
dfr_b2 %>% dplyr::arrange(x) %>% as.data.frame() %>% head()
```
]

---

# Decision tree on sample 1

.pull-left[
```{r}
fit_b1 <- rpart(formula = y ~ x,
             data = dfr_b1, #<<
             method = 'anova',
             control = rpart.control(
               maxdepth = 20, #<<
               minsplit = 10, #<<
               minbucket = 5, #<<
               cp = 0 #<<
               )
             )
```

<br>

On it's own, this is a .KULbginline[noisy prediction] with very .KULbginline[high variance]!
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit_b1, dfr))
```
]

---

# Decision tree on sample 2

.pull-left[
```{r}
fit_b2 <- rpart(formula = y ~ x,
             data = dfr_b2, #<<
             method = 'anova',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
               cp = 0
               )
             )
```

<br>

Again, very .KULbginline[high variance] on it's own!
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit_b2, dfr))
```
]

---

# Combining the predictions of both trees

.pull-left[
```{r}
# Predictions for the first tree
pred_b1 <- fit_b1 %>% predict(dfr)
# Predictions for the first tree
pred_b2 <- fit_b2 %>% predict(dfr)

# Average the predictions
pred <- rowMeans(cbind(pred_b1,
                       pred_b2))
```

<br>

Does it look like the prediction it's getting .KULbginline[less noisy]? 
<br> <br>
In other words: .KULbginline[is variance reducing?]
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = pred)
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]

.right-column[
.hi-pink[Q]: add a .KULbginline[third tree] to the .KULbginline[bagged ensemble] and inspect the predictions.

1. Generate a .KULbginline[bootstrap sample] of the data (note: don't use the same seed as before because your bootstrap samples will be the same).

1. Fit a .KULbginline[deep tree] to this bootstrap sample.

1. Make predictions for this tree and .KULbginline[average] with the others.
]

---

class: clear

.pull-left[
.hi-pink[Q1]: bootstrap sample with different seed
```{r}
# Generate the third bootstrapped sample
set.seed(28726)
bsample_3 <- dfr %>% nrow %>% 
                sample(replace = TRUE)
# Use the indices to sample the data
dfr_b3 <- dfr %>% dplyr::slice(bsample_3)
```
.hi-pink[Q2]: fit a deep tree
```{r}
# Fit an unpruned tree
fit_b3 <- rpart(formula = y ~ x,
             data = dfr_b3,
             method = 'anova',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
               cp = 0))
```
]

.pull-right[
.hi-pink[Q3]: average the predictions
```{r}
# Predictions for the third tree
pred_b3 <- fit_b3 %>% predict(dfr)
# Average the predictions
pred_new <- rowMeans(cbind(pred_b1,
                           pred_b2,
                           pred_b3))
```
]

---

class: clear

.pull-left[
Bagged ensemble with .hi-pink[B = 2]
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = pred)
```

]

.pull-right[
Bagged ensemble with .hi-pink[B = 3]
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = pred_new)
```
]

.KULbginline[Little] variance reduction might be visible, but we clearly need .KULbginline[a lot more trees]. Let's use the {ipred} package for this!

---

# Using {ipred}

```{r, eval=FALSE}
bagging(formula, data, control = rpart.control(___),
        nbagg, ns, coob)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...*

* `data`: the observation data containing the response and features

* `control`: options to pass to `rpart.control` for the .KULbginline[base learners]
  
* `nbagg`: the number of bagging iterations .KULbginline[B], i.e., the number of trees in the ensemble

* `ns`: number of observations to draw for the bootstrap samples (often less than N to save computational time)

* `coob`: a logical indicating whether an .KULbginline[out-of-bag] estimate of the error rate should be computed.

---

# Out-of-bag (OOB) error

.pull-left[
Bootstrap samples are constructed .KULbginline[with] replacement.

Some observations are .KULbginlnine[not present] in a bootstrap sample: 
  + they are called the .KULbginline[out-of-bag] observations
  + use those to calculate the out-of-bag (OOB) error
  + measures .KULbginline[hold-out] error like cross-validation.

Advantage of OOB over cross-validation?
  + the OOB error comes .KULbginline[for free] with bagging.
]

---


# Out-of-bag (OOB) error

.pull-left[
Bootstrap samples are constructed .KULbginline[with] replacement.

Some observations are .KULbginlnine[not present] in a bootstrap sample: 
  + they are called the .KULbginline[out-of-bag] observations
  + use those to calculate the out-of-bag (OOB) error
  + measures .KULbginline[hold-out] error like cross-validation.

Advantage of OOB over cross-validation?
  + the OOB error comes .KULbginline[for free] with bagging.
]
]

.pull-right[
But, is this a .KULbginline[representative] sample?
```{r}
set.seed(12345)
N <- 100000 ; x <- 1:N
mean(x %in% sample(N,
                   replace = TRUE))
```

Roughly .KULbginline[37%] of observations are OOB when N is large.

Even more when we sample .KULbginline[< N] observations
```{r}
mean(x %in% sample(N,
                   size = 0.75*N, #<<
                   replace = TRUE))
```
]

---

# Bagging properly

.pull-left[
```{r}
library(ipred)
set.seed(83946) # reproducibility

# Fit a bagged tree model
fit <- ipred::bagging(formula = y ~ x,
               data = dfr,
               nbagg = 200, #<<
               ns = nrow(dfr), #<<
               coob = TRUE, #<<
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 40,
                 minbucket = 20,
                 cp = 0) #<<
               )
```
```{r, eval = FALSE}
# Predict from this model
pred <- predict(fit, dfr)
```

With 200 trees we can see the .KULbginline[variance reduction]! 
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit, dfr))
```
]

---

# Evolution of the OOB error

.pull-left[
```{r}
set.seed(98765) # reproducibility
# Define a grid for B
nbags <- 10*(1:20) #<<
oob <- rep(0, length(nbags)) #<<
# Fit a bagged tree model
for(i in 1:length(nbags)){
  fit <- ipred::bagging(formula = y ~ x,
               data = dfr,
               nbagg = nbags[i], #<<
               ns = nrow(dfr),
               coob = TRUE, #<<
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 40,
                 minbucket = 20,
                 cp = 0)
               )
  oob[i] <- fit$err #<<
}
```
]

.pull-right[
```{r, echo = FALSE}
ggplot(data.frame('B' = nbags, 'RMSE' = oob), aes(x = B, y = RMSE)) + geom_line()
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
Use {ipred} to fit a .KULbginline[bagged] tree ensemble for the toy .hi-pink[classification] problem with data `dfc`. <br> <br>
.hi-pink[Q]: experiment with the `nbagg` and `control` parameters to see their effect on the predictions. <br>

]

---

class: clear

.hi-pink[Q]: the following parameter settings seem to produce a decent fit.

.pull-left[
```{r}
set.seed(98765) # reproducibility

# Fit a bagged tree model
fit <- ipred::bagging(formula = y ~ x1 + x2,
               data = dfc, #<<
               nbagg = 100, #<<
               ns = nrow(dfc), #<<
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 10,
                 minbucket = 5,
                 cp = 0)
               )
```
```{r, eval = FALSE}
# Predict from this model
pred <- predict(fit,
                newdata = dfc,
                type = 'class', #<<
                aggregation = 'majority' #<<
                )
```
]

.pull-right[
```{r, echo=FALSE}
plot_pred_class(dt = dfc, preds = predict(fit, dfc, type = 'class', aggregation = 'majority'))
```
]

---

class: inverse, center, middle
name: bag

# From bagging to random forests

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Problem of dominant features

A downside of bagging is that .KULbginline[dominant features] can cause individual trees to have a .KULbginline[similar structure]
  
  + known as .KULbginline[tree correlation]

--

Remember the .hi-pink[feature importance] results discussed earlier for the MTPL data? 
  
  + `bm` is a very dominant variable
  + `ageph` was rather important
  + `power` also, but to a lesser degree.

--

Problem?
  
  + bagging gets its predictive performance from .hi-pink[variance reduction]
  + however, this reduction .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] when tree correlation .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`]
  + dominant features therefore .hi-pink[hurt] the preditive performance of a bagged ensemble!

--

Solution?
  
  + .KULbginline[Random forest].
  
---

# Random forest

.KULbginline[Random forest] is a modification on bagging to get an ensemble of .hi-pink[de-correlated] trees.

--

Process is very similar to bagging, with one small .KULbginline[trick]:
  + before each split, select a .hi-pink[subset of features] at random as candidate features for splitting
  + this essentially .KUlbginline[decorrelates] the trees in the ensemble, improving predictive performance
  + the number of candidates is typically considered a .KUlbginline[tuning parameter]

--

.KULbginline[Bagging] introduces randomness in the .hi-pink[rows] of the data.

--

.KULbginline[Random forest] introduces randomness in the .KULbginline[rows] and .KULbginline[columns] of the data.

--

Many .hi-pink[packages] available, but a couple of popular ones: 
  + {randomForest}: standard for regression and classification, but not very fast
  + {randomForestSRC}: fast OpenMP implementation for survival, regression and classification
  + {ranger}: fast C++ implementation for survival, regression and classification.

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
.hi-pink[Q]: let's investigate the issue of dominant features.

1. Take .KULbginline[two bootstrap samples] from the .KULbginline[MTPL] data.

1. Fit a regression tree of .KULbginline[depth = 3] to each sample.

1. Check the resulting tree structures.
]

---

class:clear

.pull-left[
.hi-pink[Q1]: two bootstrap samples

```{r}
set.seed(486291) # reproducibility

# Generate the first bootstrapped sample
bsample_1 <- mtpl %>% nrow %>% 
                sample(replace = TRUE)

# Generate another bootstrapped sample
bsample_2 <- mtpl %>% nrow %>% 
                sample(replace = TRUE)

# Use the indices to sample the data
mtpl_b1 <- mtpl %>% dplyr::slice(bsample_1)
mtpl_b2 <- mtpl %>% dplyr::slice(bsample_2)
```
]

.pull-right[
.hi-pink[Q2]: Poisson regression tree for each sample
```{r}
fit_b1 <- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl_b1,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               minsplit = 2000,
               minbucket = 1000,
               cp = 0))

fit_b2 <- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl_b2,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               minsplit = 2000,
               minbucket = 1000,
               cp = 0))
```
]

---

class:clear

.hi-pink[Q3]: the resulting tree structures
.pull-left[
```{r, fig.width=8, echo=FALSE}
rpart.plot(fit_b1, cex = 1.4, extra = 0)
```
]

.pull-right[
```{r, fig.width=8, echo=FALSE}
rpart.plot(fit_b2, cex = 1.4, extra = 0)
```
]

---

# Using {ranger}

```{r, eval=FALSE}
ranger(formula, data, num.trees, mtry, min.node.size, max.depth,
       replace, sample.fraction, oob.error, num.threads, seed)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...*

* `data`: the observation data containing the response and features

* `num.trees`: the number of .hi-pink[trees] in the ensemble

* `mtry`: the number of .hi-pink[candidate] features for splitting

* `min.node.size` and `max.depth`: minimal leaf node size and maximal depth for the individual trees

*  `replace` and `sample.fraction`: sample with/without replacement and fraction of observations to sample

* `oob.error`: boolean indication to calculate the .hi-pink[OOB] error

* `num.threads` and `seed`: number of threads and random seed.

---

# Tuning strategy for random forests

.pull-left[
Many .KULbginline[tuning] parameters in a .hi-pink[random forest]:
  + number of trees
  + number of candidates for splitting
  + max tree depth
  + minimun leaf node size
  + sample fraction.

Construct a full .KULbginline[Cartesian] grid via `expand.grid`:

```{r}
search_grid <- expand.grid(
  num.trees = c(100,200),
  mtry = c(3,6,9),
  min.node.size = c(0.001,0.01)*nrow(mtpl),
  error = NA
  )
```

]

.pull-right[
```{r}
print(search_grid)
```
]

---

# Tuning strategy for random forests (cont.)

.pull-left[
Perform a .KULbginline[grid search] and track the .KULbginline[OOB error]:
```{r}
library(ranger)
for(i in seq_len(nrow(search_grid))) {
  # fit a random forest for the ith combination
  fit <- ranger(
    formula = nclaims ~
              ageph + agec + bm + power + 
              coverage + fuel + sex + fleet + use, 
    data = mtpl, 
    num.trees = search_grid$num.trees[i], #<<
    mtry = search_grid$mtry[i], #<<
    min.node.size = search_grid$min.node.size[i], #<<
    replace = TRUE,
    sample.fraction = 0.75,
    verbose = FALSE,
    seed = 54321
  )
  # get the OOB error 
  search_grid$error[i] <- fit$prediction.error #<<
}
```
]

.pull-right[
```{r}
search_grid %>% arrange(error)
```

What does the prediction error .KULbginline[measure] actually? <br>
The .hi-pink[Mean Squared Error], but does that make sense for us?
]

---

# Random forests for actuaries <img src="img/distRforest.png" class="title-hex">
  
All available random forest packages only support .KULbginline[standard regression] based on the .hi-pink[Mean Squared Error]
  
  + no Poisson, Gamma or log-normal loss functions available
  + bad news for actuaries.

--
  
Possible solution: the {distRforest} package on Roel's [GitHub]([https://github.com/henckr/distRforest) .font150[`r fa(name = 'github', fill = KULbg)`]
  
  + based on {rpart} which supports .KULbginline[Poisson] regression (as we have seen before)
  + extended to support .KULbginline[Gamma] and .KULbginline[log-normal] deviance as loss function
  + extended to support .KULbginline[random forest] generation
  + this package is used in [Henckaerts et al. (2020)](https://arxiv.org/abs/1904.10890).

---

class: inverse, center, middle
name: bag

# (Stochastic) Gradient Boosting Machines

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

class: clear
background-image: url(img/boosting.jpg)
background-size: contain

---

# Boosting vs. bagging

Similar to bagging, boosting is a .KULbginline[general technique] to create an .KULbginline[ensemble] of any type of base learner.

--

However, there are some .KULbginline[key differences] between both approaches:

.pull-left[

With .KULbginline[bagging]:

* .KULbginline[Strong base learners]
  + low bias, high variance
  + for example: deep trees
* .KULbginline[Variance reduction] in ensemble through .KULbginline[averaging]
* .KULbginline[Parallel] approach
  + trees not using information from each other
  + performance thanks to .hi-pink[averaging]
  + low risk for overfitting.
]

.pull-right[

With .KULbginline[boosting]:

* .KULbginline[Weak base learners]
  + low variance, high bias
  + for example: stumps
* .KULbginline[Bias reduction] in ensemble through .KULbginline[updates]
* .KULbginline[Sequential] approach
  + current tree uses information from all past trees
  + performance thanks to .hi-pink[rectifying] past mistakes
  + high risk for overfitting.
]

---

# GBM: stochastic gradient boosting with trees

.left-column[
We focus on .KULbginline[GBM]:
  + stochastic gradient boosting with .KULbginline[decision trees]
  + stochastic: .KULbginline[subsampling] in the rows (and columns) of the data
  + gradient: optimizing the loss function via .KULbginline[gradient descent].
]

.right-column[

```{r out.width= "85%", echo=FALSE}
knitr::include_graphics("img/gradient_descent.png")
```
<br> Figure 12.3 from Boehmke & Greenwell [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/gbm.html).
]

---

# Stochastic gradient descent

The .KULbginline[learning rate] (also called step size) is very important in gradient descent
  + too big: likely to .hi-pink[overshoot] the optimal solution
  + too small: .hi-pink[slow] process to reach the optimal solution
  
.center[
```{r out.width= "80%", echo=FALSE}
knitr::include_graphics("img/learning_rate.png")
```
<br> Figure 12.4 from Boehmke & Greenwell [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/gbm.html).
]

--

.KULbginline[Subsampling] allows to escape plateaus or local minima for .hi-pink[non-convex] loss functions.

---

# GBM training process

.KULbginline[Initialize] the model fit with a global average and calculate .hi-pink[pseudo-residuals].

--

Do the following .KULbginline[B] times:
  + fit a tree of a pre-specified depth to the .KULbginline[pseudo-residuals]
  + .KULbginline[update] the model fit and pseudo-residuals with a .KULbginline[shrunken] version 
  + shrinkage to slow down learning and .KULbginline[prevent] overfitting.

--

The model fit after B iterations is the .KULbginline[end product].

--

Some .KULbginline[popular] packages for stochastic gradient boosting 
  + {gbm}: standard for regression and classification, but not the fastest
  + {gbm3}: faster version of {gbm} via parallel processing, but not backwards compatible
  + {xgboost}: efficient implementation with some .hi-pink[extra] elements, for example regularization.

---

# Using {gbm}

```{r, eval=FALSE}
gbm(formula, data, distribution, var.monotone, n.trees,
    interaction.depth, shrinkage, n.minobsinnode, bag.fraction, cv.folds)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .font200[`r fa(name = 'bullhorn', fill = KULbg)`] can contain an .hi-pink[offset]!

* `data`: the observation data containing the response and features

* `distribution`: a string specifying which .hi-pink[loss function] to use (gaussian, laplace, tdist, bernoulli, poisson, coxph,...)

* `var.monotone`: vector indicating a monotone increasing (+1), decreasing (-1), or arbitrary (0) relationship

* `n.trees`: the number of .hi-pink[trees] in the ensemble

* `interaction.depth` and `n.minobsinnode`: the maximum tree .hi-pink[depth] and minimum number of leaf node observations

* `shrinkage`: shrinkage parameter applied to each tree in the expansion (also called: .hi-pink[learning rate] or step size)
  
* `bag.fraction`: fraction of observations to sample for building the next tree

* `cv.folds`: number of cross-validation folds to perform.

---

# GBM parameters

A lot of parameters at our disposal to .KULbginline[tweak] the GBM.

--

Some have a .KULbginline[big impact] on the performance and should therefore be .KULbginline[properly tuned]
  + `n.trees`: depends very much on the .hi-pink[use case], ranging from 100's to 10 000's
  + `interaction.depth`: .hi-pink[low] values are preferred for boosting to obtain weak base learners
  + `shrinkage`: typically set to the lowest possible value that is .hi-pink[computationally] feasible.

--

.hi-pink[Rule of thumb]: if `shrinkage` .font150[`r fa(name = 'arrow-circle-down', fill = KULbg)`] then `ntrees` .font150[`r fa(name = 'arrow-circle-up', fill = KULbg)`].

--

Let's have a look at the .KULbginline[impact] of these tuning parameters.

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[stumps], .hi-pink[without] applying shrinkage:
```{r}
library(gbm)
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10, #<<
           interaction.depth = 1, #<<
           shrinkage = 1 #<<
           ) 

# Predict from the GBM
pred <- predict(fit,
                n.trees = fit$n.trees, #<<
                type = 'response')
```
]

.pull-right[
```{r, echo=FALSE}
preds <- do.call(rbind, lapply(0:fit$n.trees,
                               function(i) dfr %>% mutate(iter = i,
                                                          pred = predict(fit, n.trees = i, type = 'response'))))

preds %>% ggplot(aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = m), colour = 'darkgreen', size = 1.5) +
  geom_line(aes(y = pred), colour = 'darkred', size = 1.5) + 
  transition_states(iter, transition_length = 0.1, state_length = 0.5) + labs(title = "Iteration: {closest_state}")
```
]

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[stumps], .hi-pink[with] shrinkage:
```{r}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
           interaction.depth = 1,
           shrinkage = 0.1 #<<
           ) 
```

Applying shrinkage .KULbginline[slows down] the learning process: <br>

* .hi-pink[avoids] overfitting <br>
* but we need more trees and .hi-pink[longer] training time.
]



.pull-right[
```{r, echo=FALSE}
preds <- do.call(rbind, lapply(0:fit$n.trees,
                               function(i) dfr %>% mutate(iter = i,
                                                          pred = predict(fit, n.trees = i, type = 'response'))))

preds %>% ggplot(aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = m), colour = 'darkgreen', size = 1.5) +
  geom_line(aes(y = pred), colour = 'darkred', size = 1.5) + 
  transition_states(iter, transition_length = 0.1, state_length = 0.5) + labs(title = "Iteration: {closest_state}")
```
]


---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[shallow] trees, .hi-pink[with] shrinkage:
```{r}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
           interaction.depth = 3, #<<
           shrinkage = 0.1
           ) 
```

Increasing tree .KULbginline[depth] allows more versatile splits: <br>

* .hi-pink[faster] learning <br>
* risk of .hi-pink[overfitting] (shrinkage important!) <br>

.font200[`r fa(name = 'bullhorn', fill = KULbg)`] `interaction.depth > 1` allows for .KULbginline[interactions]!
]

.pull-right[
```{r, echo=FALSE}
preds <- do.call(rbind, lapply(0:fit$n.trees,
                               function(i) dfr %>% mutate(iter = i,
                                                          pred = predict(fit, n.trees = i, type = 'response'))))

preds %>% ggplot(aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = m), colour = 'darkgreen', size = 1.5) +
  geom_line(aes(y = pred), colour = 'darkred', size = 1.5) + 
  transition_states(iter, transition_length = 0.1, state_length = 0.5) + labs(title = "Iteration: {closest_state}")
```
]

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[shallow] trees, .hi-pink[without] shrinkage:
```{r}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
           interaction.depth = 3,
           shrinkage = 1 #<<
           ) 
```

The .hi-pink[danger] for overfitting is real! <br>

.KULbginline[Rule of thumb]: 

* set `shrinkage <= 0.01` and adjust `n.trees` accordingly (.hi-pink[computational constraint]).
]

.pull-right[
```{r, echo=FALSE}
preds <- do.call(rbind, lapply(0:fit$n.trees,
                               function(i) dfr %>% mutate(iter = i,
                                                          pred = predict(fit, n.trees = i, type = 'response'))))

preds %>% ggplot(aes(x = x)) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = m), colour = 'darkgreen', size = 1.5) +
  geom_line(aes(y = pred), colour = 'darkred', size = 1.5) + 
  transition_states(iter, transition_length = 0.1, state_length = 0.5) + labs(title = "Iteration: {closest_state}")
```
]

---

# Adding trees to the ensemble

.pull-left[
Fit a GBM of .KULbginline[300] shallow trees, with shrinkage:
```{r}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 300, #<<
           interaction.depth = 3,
           shrinkage = 0.01
           ) 
```

.font200[`r fa(name = 'smile-beam', fill = KULbg)`] Look at that nice fit! <br>

.font200[`r fa(name = 'bullhorn', fill = KULbg)`] Always .KULbginline[beware] of .hi-pink[overfitting] when adding trees!
]

.pull-right[
```{r, echo=FALSE}
plot_pred_reg(dt = dfr, preds = predict(fit, n.trees = fit$n.trees, type = 'response'))
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
.hi-pink[Q]: use the previous code to .hi-pink[experiment] with your .KULbginline[GBM parameters] of choice (see `?gbm`).
]

---

class:clear

.pull-left[
Monotonic increasing fit via `var.monotone = 1`:

```{r, fig.width=8, echo=FALSE}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 300,
           interaction.depth = 3,
           shrinkage = 0.1,
           var.monotone = 1
           )
plot_pred_reg(dt = dfr, preds = predict(fit, n.trees = fit$n.trees, type = 'response'))
```
]

.pull-right[
Monotonic decreasing fit via `var.monotone = -1`:

```{r, fig.width=8, echo=FALSE}
# Fit the GBM
fit <- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 300,
           interaction.depth = 3,
           shrinkage = 0.1,
           var.monotone = -1
           )
plot_pred_reg(dt = dfr, preds = predict(fit, n.trees = fit$n.trees, type = 'response'))
```
]

---

class: inverse, center, middle
name: bag

# Tuning

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Classification with {gbm}

Let's .KULbginline[experiment] with the classification example (`data = dfc`) to get a grip on the tuning of .hi-pink[GBM parameters]

--

Which `distribution` to specify for .KULbginline[classification]? <br>
Possible .hi-pink[candidates] are:
* `"bernoulli"`: logistic regression for 0-1 outcomes
* `"huberized"`: huberized hinge loss for 0-1 outcomes
* `"adaboost"`: the AdaBoost exponential loss for 0-1 outcomes 

--

.KULbginline[Watch out]: gbm does not take factors as response so you need to .hi-pink[recode y]
  + either to a .hi-pink[numeric] in the range [0,1]
  + or a .hi-pink[boolean] `TRUE`/`FALSE`

```{r}
dfc <- dfc %>% dplyr::mutate(y_recode = as.integer(as.character(y)))
```

---

# Different parameter combinations

Set up a grid for the parameters and list to save results:
```{r}
ctrl_grid <- expand.grid(depth = c(1,3,5),
                         shrinkage = c(0.01,0.1,1))
results <- vector('list', length = nrow(ctrl_grid))
```

--

Fit different a GBM with 100 trees for each parameter combination:
```{r}
for(i in seq_len(nrow(ctrl_grid))) {
  fit <- gbm(y_recode ~ x1 + x2,
             data = dfc, #<<
             distribution = 'bernoulli', #<<
             n.trees = 100, #<<
             interaction.depth = ctrl_grid$depth[i], #<<
             shrinkage = ctrl_grid$shrinkage[i]) #<<
  
  # Save predictions, both the probabilities and the class
  results[[i]] <- dfc %>% mutate(
    depth = factor(paste('depth =',ctrl_grid$depth[i]), ordered =TRUE),
    shrinkage = factor(paste('shrinkage =',ctrl_grid$shrinkage[i]), ordered = TRUE),
    pred_prob = predict(fit, n.trees = fit$n.trees, type = 'response'), #<<
    pred_clas = factor(1*(predict(fit, n.trees = fit$n.trees, type = 'response') >= 0.5))) #<<
}
```

---

# Resulting fits

.pull-left[
The predicted .hi-pink[probabilities] 
```{r, echo=FALSE}
results <- do.call(rbind, results)
results %>% ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = pred_prob)) +
  facet_grid(depth ~ shrinkage)
```
]

.pull-right[
The predicted .hi-pink[classes]
```{r, echo = FALSE}
results %>% ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = pred_clas)) +
  facet_grid(depth ~ shrinkage)
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
.hi-pink[Q]: complete the code below to find the .hi-pink[optimal combination] of .KULbginline[tuning parameters].

1. Set up a search grid.

1. Fit a GBM for each combination op parameters.

1. Extract the OOB performance for each GBM. <br> .KULbginline[Beware:] a fitted `gbm` object returns the .hi-pink[improvements] in OOB error via `$oobag.improve`.

```{r, eval=FALSE}
my_grid <- expand.grid(___)
my_grid <- my_grid %>% dplyr::mutate(oob_improv = NA)

for(i in seq_len(nrow(my_grid))) {
  fit <- gbm(y_recode ~ x1 + x2,
             data = dfc,
             distribution = 'bernoulli',
             n.trees = ___,
             interaction.depth = ___,
             shrinkage = ___,
             ___)
  my_grid$oob_improv[i] <- sum(fit$oobag.improve)
}
```

]

---

class:clear

.pull-left[
Performing the grid search:
```{r}
my_grid <- expand.grid(depth = c(1,3,5),
                       shrinkage = c(0.01,0.1,1))
my_grid <- my_grid %>% dplyr::mutate(oob_improv = NA)

for(i in seq_len(nrow(my_grid))) {
  fit <- gbm(y_recode ~ x1 + x2,
             data = dfc,
             distribution = 'bernoulli',
             n.trees = 100,
             interaction.depth = my_grid$depth[i],
             shrinkage = my_grid$shrinkage[i])
  my_grid$oob_improv[i] <- sum(fit$oobag.improve)
}
```
]

.pull-right[
The results:
```{r}
my_grid %>% dplyr::arrange(desc(oob_improv))
```
]

<br>

Another tuning option is to set `cv.folds > 0` and track the .KULbginline[cross-validation] error via `fit$cv.error`. <br> <br>
That would be a more general approach but also .hi-pink[more time-consuming].

---

class: inverse, center, middle
name: bag

# Claim frequency and severity modeling with {gbm}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Claim frequency modeling

.pull-left[
```{r}
set.seed(76539) # reproducibility
fit <- gbm(formula = nclaims ~ 
              ageph + agec + bm + power + 
              coverage + fuel + sex + fleet + use + 
              offset(log(expo)), #<<
            data = mtpl,
            distribution = 'poisson', #<<
            var.monotone = c(0,0,1,0,0,0,0,0,0), #<<
            n.trees = 200,
            interaction.depth = 3,
            n.minobsinnode = 1000,
            shrinkage = 0.1,
            bag.fraction = 0.75, #<<
            cv.folds = 0
           )
```
]

.pull-right[
- Include the log of exposure as an .KULbginline[offset].

- Specify the .KULbginline[Poisson] distribution for the target.

- Impose a .KULbginline[monotonically increasing] constraint on `bm`.

- Perform .KULbginline[stochastic] gradient boosting with `bag.fraction < 1`.
]
---

# Inspecting the individual trees
  
```{r}
fit %>% 
  pretty.gbm.tree(i.tree = 1) %>% #<<
  print(digits = 4)
```

* Does not look that .hi-pink[pretty] right?!

--

* Even if this was a nicer representation, .KULbginline[single trees] are .hi-pink[not] going to carry much information

--

* Luckily we know some .hi-pink[tools] to get a better .KULbginline[understanding] of the GBM fit!

---

# Feature importance

Applying the `summary` function on a object of class `gbm` shows built-in feature importance results:

.pull-left[
```{r}
summary(fit, plotit = FALSE)
```
]

.pull-right[
```{r,fig.height=7.5, echo=FALSE}
invisible(summary(fit))
```
]

---

# Partial dependence plot

.pull-left[
Use the following .hi-pink[helper function] for the pdps:
```{r}
pred.fun <- function(object,newdata){
  mean(predict(object, newdata,
               n.trees = object$n.trees,
               type = 'response'))
} 
```

Partial dependence of the .hi-pink[bonus-malus level]:
```{r, eval = FALSE}
set.seed(48927)
pdp_ids <- mtpl %>% nrow %>% sample(size = 5000)
fit %>% 
  partial(pred.var = 'bm', #<<
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,],
          recursive= FALSE) %>% 
  autoplot()
```

Notice that the monotonic constraint is satisfied.
]

.pull-right[
```{r, echo = FALSE}
set.seed(48927)
pdp_ids <- mtpl %>% nrow %>% sample(size = 5000)
fit %>% 
  partial(pred.var = 'bm', #<<
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,],
          recursive= FALSE) %>% 
  autoplot()
```
]

---

# The age effect in a single tree and a gbm

.pull-left[
```{r, echo = FALSE}
pred.fun.tree <- function(object,newdata){
  mean(predict(object, newdata))
} 
fit_srt %>% 
  partial(pred.var = 'ageph', #<<
          pred.fun = pred.fun.tree,
          train = mtpl[pdp_ids,]) %>% 
  autoplot()
```
]

.pull-right[
```{r, echo = FALSE}
fit %>% 
  partial(pred.var = 'ageph', #<<
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,],
          recursive= FALSE) %>% 
  autoplot()
```
]

---

# Claim severity modeling

From the `gbm` help on the `distribution` argument:

> Currently available options are "gaussian" (squared error), "laplace" (absolute loss), "tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1 outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), "adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson" (count outcomes), "coxph" (right censored observations), "quantile", or "pairwise" (ranking measure using the LambdaMart algorithm)

Which to choose for .KULbginline[claim severity]?

--

Possible solution: the {gbm} version on Harry Southworth's [GitHub]([https://github.com/harrysouthworth/gbm) .font150[`r fa(name = 'github', fill = KULbg)`]

```{r, eval=FALSE}
install.packages("devtools")
devtools::install_github("harrysouthworth/gbm")
```

---

class: inverse, center, middle
name: bag

# XGBoost

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# XGBoost

.KULbginline[XGBoost] stands for e.hi-pink[X]treme .hi-pink[G]radient .hi-pink[Boost]ing.

--

Optimized gradient boosting library: efficient, flexible and portable across multiple languages.

--

XGBoost follows the same general boosting approach as GBM, but adds some .KULbginline[extra elements]:
  + .hi-pink[regularization]: extra protection against overfitting (see Lasso and glmnet on Day 1)
  + .hi-pink[early stopping]: stop model tuning when improvement slows down
  + .hi-pink[parallel processing]: can deliver huge speed gains
  + different .hi-pink[base learners]: boosted GLMs are a possibility
  + multiple .hi-pink[languages]: implemented in R, Python, C++, Java, Scala and Julia

--

XGBoost also allows to .KULbginline[subsample columns] in the data, much like the random forest did
  + GBM only allowed subsampling of rows
  + XGBoost therefore .hi-pink[unites] boosting and random forest to some extent.

--

Very .KULbginline[flexible] method with many many parameters, full list can be found [here](https://xgboost.readthedocs.io/en/latest/parameter.html).  

---

# Using {xgboost}

```{r, eval=FALSE}
xgboost(data, nrounds, early_stopping_rounds, params)
```

* `data`: training data, preferably an `xgb.DMatrix` (also accepts `matrix`, `dgCMatrix`, or name of a local data file) 
* `nrounds`: max number of boosting .hi-pink[iterations]
* `early_stopping_rounds`: training with a validation set will .hi-pink[stop] if the performance doesn’t improve for k rounds
* `params`: the list of .KULbginline[parameters]
  + `booster`: gbtree, gblinear or dart
  + `objective`: reg:squarederror, binary:logistic, count:poisson, survival:cox, reg:gamma, reg:tweedie, ...
  + `eval_metric`: rmse, mae, logloss, auc, poisson-nloglik, gamma-nloglik, gamma-deviance, tweedie-nloglik, ...
  + `base_score`: initial prediction for all observations (global bias)
  + `nthread`: number of parallel threads used to run XGBoost (defaults to max available)
  + `eta`: .hi-pink[learning rate] or step size used in update to prevent overfitting
  + `gamma`: minimum loss reduction required to make a further partition on a leaf node
  + `max_depth` and `min_child_weight`: maximum depth and minimum leaf node observations
  + `subsample` and `colsample_by*`: subsample rows and columns (bytree, bylevel or bynode)
  + `lambda` and `alpha`: L2 an L1 .hi-pink[regularization] term to prevent overfitting
  + `monotone_constraints`: constraint on variable monotonicity

---

# Supplying the data to XGBoost

```{r, eval=FALSE}
xgb.DMatrix(data, info = list())
```

* `data`: a `matrix` object
* `info`: a named list of additional information

--

```{r, cache=FALSE}
library(xgboost)
mtpl_xgb <- xgb.DMatrix(data = mtpl %>% 
                          select(ageph,power,bm,agec,coverage,fuel,sex,fleet,use) %>% #<<
                          data.matrix, #<<
                        info = list(
                          'label' = mtpl$nclaims, #<<
                          'base_margin' = log(mtpl$expo))) #<<

```
* Features go into the .hi-pink[data] argument (needs to be converted to a matrix)
* The target and offset are specified via `label` and `base_margin` in .hi-pink[info] respectively

--

This results in an xgb.DMatrix object:
```{r}
print(mtpl_xgb)
```


---

# A simple XGBoost model

.pull-left[
```{r, cache=FALSE, eval=FALSE}
set.seed(86493) # reproducibility
fit <- xgboost(
  data = mtpl_xgb, #<<
  nrounds = 200,
  early_stopping_rounds = 20,#<<
  verbose = FALSE,
  params = list(
    booster = 'gbtree', #<<
    objective  = 'count:poisson', #<<
    eval_metric = 'poisson-nloglik', #<<
    eta = 0.1, nthread = 1,
    subsample = 0.75, colsample_bynode = 0.5,#<<
    max_depth = 3, min_child_weight = 1000,
    gamma = 0, lambda = 1, alpha = 1 #<<
    )
  )
```
```{r,echo=FALSE}
# xgb.save(fit, fname = 'xgb_fit')
fit <-  xgb.load('xgb_fit')
```
]

.pull-right[
- Fit an XGBoost model to the .hi-pink[xgb.DMatrix] data

- Perform .hi-pink[early stopping] after 20 iterations without improvement

- Use a .hi-pink[decision tree] as base learner

- Choose the .hi-pink[Poisson] distribution for the target

- Stochastic boosting in .hi-pink[rows] and random split candidates in .hi-pink[columns] (like random forest)

- Apply .hi-pink[regularization] comparable to the elastic net penalty in {glmnet}
]

---


# Inspecting single trees
  
.pull-left[
* Possible to inspect .KULbginline[single trees] via `xgb.plot.tree`:
  + note that the trees are .hi-pink[0-indexed]
  + 0 returns first tree, 1 returns second tree,...
  + can also supply a vector of indexes
  
```{r, eval=FALSE}
xgb.plot.tree(
  feature_names = colnames(mtpl_xgb),
  model = fit,
  trees = 0
  )
```
]

.pull-right[
```{r, echo=FALSE}
xgb.plot.tree(feature_names = colnames(mtpl_xgb),
              model = fit,
              trees = 0)
```
]

---


# XGBoost in one tree

.pull-left[
* Get a .KULbginline[compressed view] of an XGBoost model via `xgb.plot.multi.trees`:
  + compressing an ensemble of trees into a single .hi-pink[tree-graph] representation
  + goal is to improve the interpretability

```{r, eval=FALSE}
xgb.plot.multi.trees(
  model = fit,
  feature_names = colnames(mtpl_xgb)
  )
```
]

.pull-right[
```{r, echo=FALSE}
xgb.plot.multi.trees(model = fit,
                     feature_names = colnames(mtpl_xgb))
```
]

---

# Further built-in interpretations

.pull-left[
* Built-in .KULbginline[feature importance]:
  + `xgb.importance`: calculates .hi-pink[data]
  + `xgb.ggplot.importance`: .hi-pink[visual] representation

```{r, eval=FALSE}
xgb.ggplot.importance( #<<
  importance_matrix = xgb.importance( #<<
    feature_names = colnames(mtpl_xgb),
    model = fit
  )
)
```

* Packages such as {vip} and {pdp} can also be used on `xgboost` models
  + even a [vignette](https://bgreenwell.github.io/pdp/articles/pdp-example-xgboost.html) dedicated to this
]

.pull-right[
```{r, echo=FALSE}
xgb.ggplot.importance(
  importance_matrix = xgb.importance(
    feature_names = colnames(mtpl_xgb),
    model = fit
  )
)
```
]

---

# Cross-validation with XGBoost

.KULbginline[Built-in cross-validation] with `xgb.cv`
  + same interface as the `xgboost` function
  + add `nfolds` to define the .hi-pink[number of folds]
  + add `stratified` for .hi-pink[stratification]

```{r}
set.seed(86493) # reproducibility
xval <- xgb.cv(data = mtpl_xgb,
               nrounds = 200,
               early_stopping_rounds = 20,
               verbose = FALSE,
               nfold = 5, #<<
               stratified = TRUE, #<<
               params = list(booster = 'gbtree',
                             objective  = 'count:poisson',
                             eval_metric = 'poisson-nloglik',
                             eta = 0.1, nthread = 1,
                             subsample = 0.75, colsample_bynode = 0.5,
                             max_depth = 3, min_child_weight = 1000,
                             gamma = 0, lambda = 1, alpha = 1))


```

---

# Cross-validation results

Get the cross-validation .KULbginline[results] via `$evaluation_log`:

```{r}
xval$evaluation_log %>% print(digits = 5)
```

---

# Cross-validation results

```{r, echo=FALSE}
xval_log <- xval$evaluation_log
xval_log <- as.data.frame(rbind(as.matrix(xval_log[,c(1,2,3)]),as.matrix(xval_log[,c(1,4,5)])))
names(xval_log) <- c('iteration','poisson_nloglik','std')
xval_log$loss <- c(rep('train',nrow(xval_log)/2),rep('test',nrow(xval_log)/2))
```

.pull-left[
```{r,echo=FALSE}
ggplot(xval_log, aes(x=iteration, y=poisson_nloglik, colour=loss, linetype = loss)) + geom_line(size = 1.3)
```
]

.pull-right[
```{r,echo=FALSE}
ggplot(xval_log[c(150:200,350:400),], aes(x=iteration, y=poisson_nloglik, colour=loss, linetype = loss)) + geom_line(size = 1.5)
# geom_errorbar(aes(ymin=poisson_nloglik-std, ymax=poisson_nloglik+std), width=.1)
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn

]


.right-column[
.KULbginline[That's a wrap] on .hi-pink[tree-based ML]! Now it's your time to experiment. <br>
Below are some .hi-pink[suggestions], but feel free to .KULbginline[get creative].

1. Perform a .hi-pink[tuning] exercise for your favorite tree-based algorithm. Beware that tuning can take up a lot of time, so do not overdo this.

1. Apply your favorite algorithm on a classification problem, for example to predict the .hi-pink[occurence] of a claim.

1. Use a .hi-pink[gamma] deviance to build a .KULbginline[severity] XGBoost model. The `mtpl` data contains the average claim amount in the feature `average`. Remember: if you want to develop a GBM with a gamma loss, you need the implementation available at Harry Southworth's [Github](https://github.com/harrysouthworth/gbm).

1. Develop a boosting or random forest model for the .hi-pink[Ames Housing] data and extract .KULbginline[insights] in the form of feature importance and partial dependence plots.

1. Compare the performance of a regression tree, random forest and boosting model. Which model performs .hi-pink[best]?


]

---

name: wrap-up

# Thanks!  <img src="img/xaringan.png" class="title-hex">

<br>
<br>
<br>
<br>

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
<br> <br> <br>
Course material available via 
<br>
`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/hands-on-machine-learning-R-module-2


```{r, eval=FALSE, include=FALSE}
# this code can be used to extract the R code from an R Markdown (Rmd) document
library(knitr)
path <- "C:/Users/u0043788/Dropbox/Workshop AG/sheets"
setwd(path)
file.exists("ML_part2.Rmd")
purl("ML_part2.Rmd")
```


