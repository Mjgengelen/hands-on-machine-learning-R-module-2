<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Hands-on Machine Learning with R - Module 2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katrien Antonio &amp; Roel Henckaerts" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/viz/viz.js"></script>
    <link href="libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
    <script src="libs/grViz-binding/grViz.js"></script>
    <link rel="stylesheet" href="css/metropolis.css" type="text/css" />
    <link rel="stylesheet" href="css/metropolis-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Hands-on Machine Learning with R - Module 2
## Hands-on webinar
<html>
<div style="float:left">

</div>
<hr align='center' color='#116E8A' size=1px width=97%>
</html>
### Katrien Antonio &amp; Roel Henckaerts
### <a href="https://github.com/katrienantonio/hands-on-machine-learning-R-module-2">hands-on-machine-learning-R-module-2</a> | January 14 &amp; 21, 2021

---






class: inverse, center, middle
name: prologue

# Prologue

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

name: introduction

# Introduction

### Course

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; https://github.com/katrienantonio/hands-on-machine-learning-R-module-2

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt; [https://katrienantonio.github.io/](https://katrienantonio.github.io/) &amp; [https://henckr.github.io/](https://henckr.github.io/)

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/&gt;&lt;/svg&gt; [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) &amp; [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 640 512"&gt;&lt;path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/&gt;&lt;/svg&gt; (Katrien) Professor in insurance data science

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 640 512"&gt;&lt;path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/&gt;&lt;/svg&gt; (Roel) PhD student in insurance data science

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  
  ```r
  version$version.string
  ## [1] "R version 4.0.3 (2020-10-10)"
  ```

☑ Do you have a fairly recent version of RStudio? 
  
  ```r
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```

☑ Have you installed the R packages listed in the software requirements? 

or

☑ Have you created an account on RStudio Cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this module .font140[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M505.05 19.1a15.89 15.89 0 0 0-12.2-12.2C460.65 0 435.46 0 410.36 0c-103.2 0-165.1 55.2-211.29 128H94.87A48 48 0 0 0 52 154.49l-49.42 98.8A24 24 0 0 0 24.07 288h103.77l-22.47 22.47a32 32 0 0 0 0 45.25l50.9 50.91a32 32 0 0 0 45.26 0L224 384.16V488a24 24 0 0 0 34.7 21.49l98.7-49.39a47.91 47.91 0 0 0 26.5-42.9V312.79c72.59-46.3 128-108.4 128-211.09.1-25.2.1-50.4-6.85-82.6zM384 168a40 40 0 1 1 40-40 40 40 0 0 1-40 40z"/&gt;&lt;/svg&gt;]

--

* develop foundations of working with .KULbginline[regression and decision trees]

--

* step from simple trees to ensembles of trees, with .KULbginline[bagging] and .KULbginline[boosting]

--

* focus on the use of these ML methods for the .KULbginline[analysis of frequency + severity data] 

--

* discuss and construct some useful .KULbginline[interpretation tools], e.g. variable importance plots, partial dependence plots.

---

# Module 2's Outline

.pull-left[

* [Prologue](#prologue)

* [Decision tree](#tree)

  - what is tree-based machine learning?
  - tree basics: structure, terminology, growing process
  - using {rpart}
  - pruning via cross-validation
  - examples on regression and classification
  - modelling claim frequency and severity data with trees
  
* [Interpretation tools](#interpret)

 - feature importance
 - partial dependence plot
 - the {vip} and {pdp} packages

]

.pull-right[

* [Bagging](#bag) 

 - from a single tree to Bootstrap Aggregating
 - out-of-bag error

* [Random forest](#rf)

 - from bagging to random forests
 - tuning

* [Gradient boosting](#gb)

 - (stochastic) gradient boosting with trees
 - training process and tuning parameters
 - using {gbm}
 - modelling claim frequencies and severities
 - using {xgboost}

]


---

name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

&lt;img src = "img/AI_ML_DL.jpg" height = "350px" /&gt;

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---

# Background reading

.left-column[

&lt;br&gt;

&lt;img src = "img/naaj.png" height = "350px" /&gt;

]

.right-column[

Henckaerts et al. (2020) paper on [Boosting insights in insurance tariff plans with tree-based machine learning methods](https://katrienantonio.github.io/publication/2020-boosting/)

- full algorithmic details of regression trees, bagging, random forests and gradient boosting machines
- with focus on claim frequency and severity modelling
- including interpretation tools (VIP, PDP, ICE, H-statistic)
- model comparison (GLMs, GAMs, trees, RFs, GBMs) 
- managerial tools (e.g. loss ratio, discrimination power).

The paper comes with two notebooks, see [examples tree-based paper](https://github.com/henckr/treeML) and [severity modelling](https://github.com/henckr/sevtree).

The paper comes with an R package for fitting random forests on insurance data, see [distRforest](https://github.com/henckr/distRforest).

]

---

# What is tree-based machine learning?

.KULbginline[Machine learning (ML)] according to [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning):

&gt; *"Machine learning algorithms build a .hi-pink[mathematical model] based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to perform the task."*

&gt; This definition goes all the way back to [Arthur Samuel](https://en.wikipedia.org/wiki/Arthur_Samuel), who coined the term "machine learning" in 1959.

--

.KULbginline[Tree-based ML] makes use of a .KULbginline[tree] as building block for the mathematical model.

&lt;img src="img/tree_based.png" width="70%" style="display: block; margin: auto;" /&gt;

--

So, a natural question to start from is: what is a .KULbginline[tree]?

---



class: inverse, center, middle
name: tree-basic

# Tree basics

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---


class: clear
background-image: url(img/decision_tree.jpg)
background-size: contain


---

# Tree structure and terminology

The top of the tree contains all available training observations: the .hi-pink[root node].

--

We .KULbginline[partition] the data into homogeneous non-overlapping subgroups: the .hi-pink[nodes]

--

We create subgroups via .KULbginline[simple yes-no questions].

--

A tree then predicts the output in a .hi-pink[leaf node] as follows:

  + average of the response for regression
  + majority voting for classification


---

# Tree structure and terminology

The top of the tree contains all available training observations: the .hi-pink[root node].


We .KULbginline[partition] the data into homogeneous non-overlapping subgroups: the .hi-pink[nodes].

&lt;img src="img/tree_example.jpg" width="50%" style="float:right; padding:10px" style="display: block; margin: auto;" /&gt;

We create subgroups via .KULbginline[simple yes-no questions].

A tree then predicts the output in a .hi-pink[leaf node] as follows:

  + average of the response for regression
  + majority voting for classification.
  
Different types of nodes:

&lt;img src="img/tree_legend.jpg" width="27%" style="display: block; margin: auto;" /&gt;


---

# Tree growing process


A golden standard is the .KULbginline[C]lassification .KULbginline[A]nd .KULbginline[R]egression .KULbginline[T]ree algorithm: .KULbginline[CART] (Breiman et al., 1984).

--

CART uses .KULbginline[binary recursive partitioning] to split the data in subgroups.

--

In each node, we search for the best feature to partition the data into two regions: R&lt;sub&gt;1&lt;/sub&gt; and R&lt;sub&gt;2&lt;/sub&gt; (hence, .KULbginline[binary]).

--

.font140[.KULbginline[Take-away]] .font160[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] &amp;nbsp; - &amp;nbsp; what is .KULbginline[best?] 

Minimize the .KULbginline[overall loss] between observed responses and leaf node prediction
  + overall loss = loss in region R&lt;sub&gt;1&lt;/sub&gt; + loss in region R&lt;sub&gt;2&lt;/sub&gt;
  + for regression: mean squared or absolute error, deviance,...
  + for classification: cross-entropy, Gini index,...

--

After splitting the data, this process is repeated for region R&lt;sub&gt;1&lt;/sub&gt; and R&lt;sub&gt;2&lt;/sub&gt; separately (hence, .KULbginline[recursive]).

--

Repeat until .KULbginline[stopping criterion] is satisfied, e.g., maximum depth of a tree or minimum loss improvement.


---

# Using {rpart}


```r
rpart(formula, data, method,
      control = rpart.control(cp, maxdepth, minsplit, minbucket))
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...* &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] &amp;nbsp;&amp;nbsp; no need to include the interactions!

* `data`: the observation data containing the response and features

* `method`: a string specifying which .hi-pink[loss function] to use
  + "anova" for regression (SSE as loss)
  + "class" for classification (Gini as loss)
  + "poisson" for Poisson regression (Poisson deviance as loss, see more later)
  
* `cp`: complexity parameter specifying the proportion by which the overall error should improve for a split to be attempted
  
* `maxdepth`: the maximum depth of the tree

* `minsplit`: minimum number of observations in a node for a split to be attempted

* `minbucket`: minimum number of observations in a leaf node.

---

class: inverse, center, middle
name: toy-regr

# Toy example of a regression tree

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# Simulated data

.pull-left[

```r
library(tidyverse)
set.seed(54321) # reproducibility
dfr &lt;- tibble::tibble(
  x = seq(0, 2 * pi, length.out = 500),
  m = 2 * sin(x),
  y = m + rnorm(length(x), sd = 1)
  )
```

```
##             x          m          y
## 1  0.00000000 0.00000000 -0.1789007
## 2  0.01259155 0.02518244 -0.9028617
## 3  0.02518311 0.05036089 -0.7336728
## 4  0.03777466 0.07553136 -1.5750691
## 5  0.05036621 0.10068985 -0.3073767
## 6  0.06295777 0.12583237 -0.9696970
## 7  0.07554932 0.15095495 -1.5412872
## 8  0.08814088 0.17605359  2.6920994
## 9  0.10073243 0.20112432  1.5964765
## 10 0.11332398 0.22616316  0.4061405
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Decision stump - a tree with only one split

.pull-left[

```r
library(rpart)
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
*            method = 'anova',
             control = rpart.control(
*              maxdepth = 1
               )
             )
print(fit)
## n= 500 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 500 1498.4570 -0.03876172  
##   2) x&gt;=2.965311 264  384.3336 -1.24604800 *
##   3) x&lt; 2.965311 236  298.8888  1.31176200 *
```

```r
library(rpart.plot) # for nice plots
*rpart.plot(fit, digits = 4, cex = 2)
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;
]

---
# Decision stump - a tree with only one split

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
*            method = 'anova',
             control = rpart.control(
*              maxdepth = 1
               )
             )
print(fit)
## n= 500 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 500 1498.4570 -0.03876172  
##   2) x&gt;=2.965311 264  384.3336 -1.24604800 *
##   3) x&lt; 2.965311 236  298.8888  1.31176200 *
```

```r
# Get predictions via the predict function
*pred &lt;- predict(fit, dfr)
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Adding splits

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
*              maxdepth = 2
               )
             )
print(fit)
## n= 500 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 500 1498.45700 -0.03876172  
##   2) x&gt;=2.965311 264  384.33360 -1.24604800  
##     4) x&gt;=3.985227 183  228.44490 -1.57111200 *
##     5) x&lt; 3.985227 81   92.86428 -0.51164310 *
##   3) x&lt; 2.965311 236  298.88880  1.31176200  
##     6) x&lt; 0.535141 43   55.23637  0.47680020 *
##     7) x&gt;=0.535141 193  206.99550  1.49779000 *
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Adding splits (cont.)

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
*              maxdepth = 2
               )
             )
print(fit)
## n= 500 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 500 1498.45700 -0.03876172  
##   2) x&gt;=2.965311 264  384.33360 -1.24604800  
##     4) x&gt;=3.985227 183  228.44490 -1.57111200 *
##     5) x&lt; 3.985227 81   92.86428 -0.51164310 *
##   3) x&lt; 2.965311 236  298.88880  1.31176200  
##     6) x&lt; 0.535141 43   55.23637  0.47680020 *
##     7) x&gt;=0.535141 193  206.99550  1.49779000 *
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
Let's get familiar with the structure of a decision tree. &lt;br&gt; &lt;br&gt;

.hi-pink[Q]: choose one of the trees from the previously discussed examples and pick a leaf node, but keep it simple for now.

1. Replicate the .KULbginline[predictions] for that leaf node, based on the split(s) and the training data.

1. Replicate the .KULbginline[deviance] measure for that leaf node, based on the split(s), the training data and your predictions from Q1. 

Hint: the deviance used in an anova {rpart} tree is the .hi-pink[Sum of Squared Errors (SSE)]:

`$$\begin{eqnarray*}
\textrm{SSE} = \sum_{i=1}^n (\color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i))^2,
\end{eqnarray*}$$`

]

---

class: clear

.pull-left[
Take for example the tree with depth two: 

```r
print(fit)
## n= 500 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 500 1498.45700 -0.03876172  
##   2) x&gt;=2.965311 264  384.33360 -1.24604800  
##     4) x&gt;=3.985227 183  228.44490 -1.57111200 *
##     5) x&lt; 3.985227 81   92.86428 -0.51164310 *
##   3) x&lt; 2.965311 236  298.88880  1.31176200  
##     6) x&lt; 0.535141 43   55.23637  0.47680020 *
##     7) x&gt;=0.535141 193  206.99550  1.49779000 *
```
Let's predict the values for leaf node 6.
]

.pull-right[
.hi-pink[Q.1]: calculate the prediction

```r
# Subset observations in node 6
obs &lt;- dfr %&gt;% dplyr::filter(x &lt; 0.535141)

# Predict
pred &lt;- obs$y %&gt;%  mean
pred
## [1] 0.4768002
```

.hi-pink[Q.2]: calculate the deviance

```r
# Deviance
dev &lt;- (obs$y - pred)^2 %&gt;% sum
dev
## [1] 55.23637
```
]


---

# A very deep tree

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 10,
*              minbucket = 5,
*              cp = 0
               )
             )
```

.font140[.KULbginline[Take-away]] .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] &amp;nbsp; - &amp;nbsp; understanding the `cp` parameter:

  - unitless in {rpart} (different from original CART)
  - `cp = 1` returns a .hi-pink[root node], without splits
  - `cp = 0` returns the .hi-pink[deepest tree possible], allowed by the other stopping criteria.
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;
]

---

# A very deep tree (cont.)

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 10,
*              minbucket = 5,
*              cp = 0
               )
             )
```

&lt;br&gt; 

What is your opinion on the tree shown on the right?

]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle
name: prune-tree

# Pruning via cross-validation in {rpart}

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

#  How deep should a tree be?

.pull-left[

The .KULbginline[bias-variance trade off]:
  - a .hi-pink[shallow] tree will underfit: 
  &lt;br&gt;
  bias .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;] and variance .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] &lt;br&gt;
  - a .hi-pink[deep] tree will overfit: 
  &lt;br&gt;
  bias .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] and variance .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;] &lt;br&gt;
  - find right .KULbginline[balance] between bias and variance!

Typical approach to get the right fit:
  - fit an overly complex .KULbginline[deep tree]
  - .KULbginline[prune] the tree to find the .KULbginline[optimal subtree].
  
How to .KULbginline[prune?]
]


---

#  How deep should a tree be?

.pull-left[

The .KULbginline[bias-variance trade off]:
  - a .hi-pink[shallow] tree will underfit: 
  &lt;br&gt;
  bias .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;] and variance .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] &lt;br&gt;
  - a .hi-pink[deep] tree will overfit: 
  &lt;br&gt;
  bias .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] and variance .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;] &lt;br&gt;
  - find right .KULbginline[balance] between bias and variance!

Typical approach to get the right fit:
  - fit an overly complex .KULbginline[deep tree]
  - .KULbginline[prune] the tree to find the .KULbginline[optimal subtree].
  
How to .KULbginline[prune?]

]


.pull-right[



Look for the smallest subtree that minimizes a .KULbginline[penalized loss function]:
  `$$\min\{f_{\textrm{loss}} + \alpha \cdot |T|\}$$`
  + loss function `\(f_{\textrm{loss}}\)`
  + complexity parameter `\(\alpha\)`
  + number of leaf nodes `\(|T|\)`.

A shallow tree results when `\(\alpha\)` is large and a deep tree when `\(\alpha\)` is small.
  
Perform .hi-pink[cross-validation] on the complexity parameter: 
  + `cp` is the complexity parameter in {rpart}
  + `cp` is `\(\alpha\)` divided by `\(f_{\text{loss}}\)` evaluated in root node.
  
Cfr. tuning of the regularization paramater in lasso from {glmnet} in .KULbginline[Module 1].
]

---

# Pruning via cross-validation

.pull-left[

```r
set.seed(87654) # reproducibility
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 10,
               minsplit = 20,
               minbucket = 10,
*              cp = 0,
*              xval = 5
               )
             )
```

```r
# Plot the cross-validation results
*plotcp(fit)
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;
]

---
 
# Pruning via cross-validation

.pull-left[

```r
set.seed(87654) # reproducibility
fit &lt;- rpart(formula = y ~ x,
             data = dfr,
             method = 'anova',
             control = rpart.control(
               maxdepth = 10,
               minsplit = 20,
               minbucket = 10,
*              cp = 0,
*              xval = 5
               )
             )
# Get xval results via 'cptable' attribute
*cpt &lt;- fit$cptable
```

```r
*print(cpt[1:20,])
# Which cp value do we choose?
min_xerr &lt;- which.min(cpt[,'xerror'])
se_rule &lt;- min(which(cpt[, 'xerror'] &lt; 
  (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
```
]

.pull-right[

```
##            CP nsplit rel error   xerror      xstd
## 1  0.54404922      0  1.000000 1.004726 0.0514072
## 2  0.04205955      1  0.455951 0.479691 0.0306899
## 3  0.02638545      2  0.413891 0.459565 0.0303987
## 4  0.02446313      3  0.387506 0.432619 0.0288631
## 5  0.01686947      4  0.363043 0.407090 0.0271596
## 6  0.00556730      5  0.346173 0.402555 0.0269263
## 7  0.00537029      6  0.340606 0.390939 0.0263032
## 8  0.00455035      7  0.335236 0.389550 0.0259170
## 9  0.00438010      8  0.330685 0.387857 0.0262972
## 10 0.00437052      9  0.326305 0.384689 0.0262569
## 11 0.00417651     11  0.317564 0.384689 0.0262569
## 12 0.00413572     12  0.313388 0.389304 0.0264134
## 13 0.00288842     13  0.309252 0.394634 0.0263896
## 14 0.00248513     14  0.306363 0.393097 0.0255738
## 15 0.00230656     16  0.301393 0.394084 0.0254549
## 16 0.00227479     17  0.299087 0.401089 0.0260820
## 17 0.00222192     18  0.296812 0.403132 0.0258395
## 18 0.00218218     19  0.294590 0.403132 0.0258395
## 19 0.00189012     20  0.292408 0.405123 0.0258289
## 20 0.00177060     21  0.290518 0.405770 0.0258239
```
]


---

# Minimal CV error or 1 SE rule

.pull-left[


```r
*fit_1 &lt;- prune(fit, cp = cpt[min_xerr, 'CP'])
```
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-34-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

.pull-right[


```r
*fit_2 &lt;- prune(fit, cp = cpt[se_rule, 'CP'])
```
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-37-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

.hi-pink[Q]: Trees are often associated with .KULbginline[high variance], meaning that the resulting model can be very sensitive to the input data. Let's explore this statement!

1. Generate a second data set `dfr2` with a different seed.

1. Fit an optimal tree to this data following the pruning strategy. 

1. Can you spot substantial differences with the trees from before?

&lt;br&gt; 

.hi-pink[Q.1]: a brand new data set


```r
# Generate the data
set.seed(83625493)
dfr2 &lt;- tibble(
  x = seq(0, 2 * pi, length.out = 500),
  m = 2 * sin(x),
  y = m + rnorm(length(x), sd = 1)
  )
```
]

---

class: clear



.pull-left[
.hi-pink[Q.2a]: optimal tree with .KULbginline[min CV error]

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-41-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.hi-pink[Q.2b]: optimal tree with .KULbginline[one SE rule]

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-43-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

&lt;br&gt;

.hi-pink[Q.3]: trees look .KULbginline[rather different] compared to those from before, even though they try to approximate the same function.


---

class: inverse, center, middle
name: toy-class

# Toy example of a classification tree

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Simulated data

.pull-left[

```r
set.seed(54321) # reproducibility
dfc &lt;- tibble::tibble(
  x1 = rep(seq(0.1,10,by = 0.1), times = 100),
  x2 = rep(seq(0.1,10,by = 0.1), each = 100),
  y = as.factor(
    pmin(1,
         pmax(0,
              round(
      1*(x1+2*x2&lt;8) + 1*(3*x1+x2&gt;30) + 
        rnorm(10000,sd = 0.5))
              )
        )
    )
)
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-45-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Fitting a simple tree 

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x1 + x2,
             data = dfc,
*            method = 'class',
             control = rpart.control(
*              maxdepth = 2
               )
             )
print(fit)
## n= 10000 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 10000 3760 0 (0.6240000 0.3760000)  
##   2) x1&lt; 8.05 8000 2454 0 (0.6932500 0.3067500)  
##     4) x2&gt;=2.65 5920 1236 0 (0.7912162 0.2087838) *
##     5) x2&lt; 2.65 2080  862 1 (0.4144231 0.5855769) *
##   3) x1&gt;=8.05 2000  694 1 (0.3470000 0.6530000)  
##     6) x2&lt; 3.95 780  306 0 (0.6076923 0.3923077) *
##     7) x2&gt;=3.95 1220  220 1 (0.1803279 0.8196721) *
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-47-1.png" width="95%" style="display: block; margin: auto;" /&gt;
]


---

# Fitting a simple tree (cont.)

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x1 + x2,
             data = dfc,
*            method = 'class',
             control = rpart.control(
*              maxdepth = 2
               )
             )
print(fit)
## n= 10000 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 10000 3760 0 (0.6240000 0.3760000)  
##   2) x1&lt; 8.05 8000 2454 0 (0.6932500 0.3067500)  
##     4) x2&gt;=2.65 5920 1236 0 (0.7912162 0.2087838) *
##     5) x2&lt; 2.65 2080  862 1 (0.4144231 0.5855769) *
##   3) x1&gt;=8.05 2000  694 1 (0.3470000 0.6530000)  
##     6) x2&lt; 3.95 780  306 0 (0.6076923 0.3923077) *
##     7) x2&gt;=3.95 1220  220 1 (0.1803279 0.8196721) *
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-49-1.png" style="display: block; margin: auto;" /&gt;
]

---

# What about an overly complex tree?

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 10,
*              minbucket = 5,
*              cp = 0
               )
             )
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;" /&gt;
]

---

# What about an overly complex tree?

.pull-left[

```r
fit &lt;- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 10,
*              minbucket = 5,
*              cp = 0
               )
             )
```
&lt;br&gt; 
Clearly .KULbginline[overfitting]!
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-53-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
Let's find a satisfying fit for this classification example. &lt;br&gt;
.hi-pink[Q]: perform .KULbginline[cross-validation] on `cp` to find the .KULbginline[optimal pruned subtree].

1. Set `xval = 5` in `rpart.control()` (do not forget to set a .hi-pink[seed] beforehand).

1. Graphically inspect the xval results via `plotcp()`.

1. Extract the xval results via `$cptable`.

1. Apply the min xerror and/or the one SE rule to find the .hi-pink[optimal] `cp`.

1. Show the resulting classifier graphically.


]

---

class: clear

.pull-left[
.hi-pink[Q.1]: fit a complex tree and perform cross-validation

```r
set.seed(87654) # reproducibility
fit &lt;- rpart(formula = y ~ x1 + x2,
             data = dfc,
             method = 'class',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
*              cp = 0,
*              xval = 5
               )
             )
```
]

.pull-right[
.hi-pink[Q.2]: inspect the xval results graphically

```r
plotcp(fit)
```

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-55-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: clear

.pull-left[
.hi-pink[Q.3]: extract the xval results in a table

```r
# Get xval results via 'cptable' attribute
cpt &lt;- fit$cptable
```
.hi-pink[Q.4]: optimal `cp` via min CV error or one SE rule

```r
# Which cp value do we choose?
min_xerr &lt;- which.min(cpt[,'xerror'])

se_rule &lt;- min(which(cpt[, 'xerror'] &lt; 
  (cpt[min_xerr, 'xerror'] + cpt[min_xerr, 'xstd'])))
```

```r
unname(min_xerr)
## [1] 29
```

```r
se_rule
## [1] 20
```


]

.pull-right[

```r
print(cpt[16:35,], digits = 6)
##             CP nsplit rel error   xerror      xstd
## 16 0.001861702     23  0.471543 0.500000 0.0103913
## 17 0.001595745     25  0.467819 0.494149 0.0103443
## 18 0.001329787     26  0.466223 0.490957 0.0103184
## 19 0.001063830     33  0.456915 0.487234 0.0102880
## 20 0.000930851     34  0.455851 0.483245 0.0102552
## 21 0.000797872     36  0.453989 0.482713 0.0102508
## 22 0.000709220     41  0.450000 0.480319 0.0102310
## 23 0.000664894     44  0.447872 0.480319 0.0102310
## 24 0.000531915     50  0.443883 0.478457 0.0102155
## 25 0.000443262     55  0.441223 0.476330 0.0101978
## 26 0.000398936     58  0.439894 0.476596 0.0102000
## 27 0.000354610     60  0.439096 0.477660 0.0102089
## 28 0.000332447     66  0.436968 0.477660 0.0102089
## 29 0.000265957     74  0.434309 0.474734 0.0101844
## 30 0.000199468    103  0.426330 0.478989 0.0102200
## 31 0.000177305    112  0.424468 0.488830 0.0103011
## 32 0.000166223    128  0.421543 0.488830 0.0103011
## 33 0.000132979    139  0.419681 0.502128 0.0104082
## 34 0.000113982    153  0.417819 0.515160 0.0105105
## 35 0.000106383    167  0.416223 0.523936 0.0105780
```
]

---

class: clear

.pull-left[
.hi-pink[Q.5a]: optimal subtree via .KULbginline[min CV error]

```r
fit_1 &lt;- prune(fit, cp = cpt[min_xerr, 'CP'])
```
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-62-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.hi-pink[Q.5b]: optimal subtree via .KULbginline[one SE rule]

```r
fit_2 &lt;- prune(fit, cp = cpt[se_rule, 'CP'])
```
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-64-1.png" width="85%" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle
name: freq-sev-tree

# Claim frequency and severity modeling with {rpart}

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---


# Claim frequency prediction on the MTPL data

Recall the MTPL data set introduced in Module 1.

The .KULbginline[Poisson GLM] is a classic approach for modelling .KULbginline[claim frequency] data.

How to deal with claim counts in a decision tree?

Use the .KULbginline[Poisson deviance] as .hi-pink[loss function]:
 
`$$\begin{eqnarray*}
D^{\textrm{Poi}} = 2 \cdot \sum_{i=1}^{n} \color{#FFA500}{y}_i \cdot \ln \frac{\color{#FFA500}{y}_i}{\textrm{expo}_i \cdot \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)} - \{\color{#FFA500}{y}_i - \textrm{expo}_i \cdot \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)\},
\end{eqnarray*}$$`

with `\(\textrm{expo}\)` the exposure measure.


--

Here we go: 


```r
# Read the MTPL data
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) 
mtpl &lt;- read.table('../data/PC_data.txt',
                   header = TRUE, stringsAsFactors = TRUE) %&gt;% 
  as_tibble() %&gt;% rename_all(tolower) %&gt;% rename(expo = exp)
```

---

# Fitting a simple tree to the MTPL data

.pull-left[

```r
fit &lt;- rpart(formula = 
*              cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
*            data = mtpl,
*            method = 'poisson',
             control = rpart.control(
*              maxdepth = 3,
               cp = 0)
             )
```

```r
print(fit)
```


.font140[.KULbginline[Take-away]] .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;]  &amp;nbsp; - &amp;nbsp; .KULbginline[Poisson tree] in {rpart}: 
  - Poisson deviance via `method = 'poisson'` &lt;br&gt;
  - response as two-column matrix: `cbind(expo,y)`.
]

.pull-right[

```
## n= 163231 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 163231 89944.320 0.13933520  
##    2) bm&lt; 6.5 127672 63455.290 0.11784050  
##      4) bm&lt; 1.5 88621 41252.130 0.10550490  
##        8) ageph&gt;=55.5 33646 14281.360 0.08899811 *
##        9) ageph&lt; 55.5 54975 26835.800 0.11598320 *
##      5) bm&gt;=1.5 39051 21872.010 0.14641040  
##       10) ageph&gt;=57.5 8463  4324.098 0.11963920 *
##       11) ageph&lt; 57.5 30588 17496.720 0.15408620 *
##    3) bm&gt;=6.5 35559 24843.720 0.22188630  
##      6) bm&lt; 10.5 22657 15022.440 0.19808030  
##       12) ageph&gt;=26.5 17196 10950.970 0.18565170 *
##       13) ageph&lt; 26.5 5461  4025.443 0.23668440 *
##      7) bm&gt;=10.5 12902  9678.292 0.26753260  
##       14) agec&lt; 6.5 4472  3181.981 0.23435030 *
##       15) agec&gt;=6.5 8430  6471.783 0.28640140 *
```
]

---

# Fitting a simple tree to the MTPL data

.pull-left[

```r
fit &lt;- rpart(formula = 
*              cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
*            data = mtpl,
*            method = 'poisson',
             control = rpart.control(
*              maxdepth = 3,
               cp = 0)
             )
```

```r
print(fit)
```
&lt;br&gt;
Easier way to .KULbginline[understand] this tree? &lt;br&gt;
Try `rpart.plot` from the package .KULbginline[{rpart.plot}]

]

.pull-right[

```
## n= 163231 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 163231 89944.320 0.13933520  
##    2) bm&lt; 6.5 127672 63455.290 0.11784050  
##      4) bm&lt; 1.5 88621 41252.130 0.10550490  
##        8) ageph&gt;=55.5 33646 14281.360 0.08899811 *
##        9) ageph&lt; 55.5 54975 26835.800 0.11598320 *
##      5) bm&gt;=1.5 39051 21872.010 0.14641040  
##       10) ageph&gt;=57.5 8463  4324.098 0.11963920 *
##       11) ageph&lt; 57.5 30588 17496.720 0.15408620 *
##    3) bm&gt;=6.5 35559 24843.720 0.22188630  
##      6) bm&lt; 10.5 22657 15022.440 0.19808030  
##       12) ageph&gt;=26.5 17196 10950.970 0.18565170 *
##       13) ageph&lt; 26.5 5461  4025.443 0.23668440 *
##      7) bm&gt;=10.5 12902  9678.292 0.26753260  
##       14) agec&lt; 6.5 4472  3181.981 0.23435030 *
##       15) agec&gt;=6.5 8430  6471.783 0.28640140 *
```
]

---

# Fitting a simple tree to the MTPL data

.center[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-72-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]

.right-column[
Verify whether the .KULbginline[prediction] in a leaf node is .hi-pink[what you would expect]. &lt;br&gt;

.hi-pink[Q]: take the leftmost node as an example: `bm &lt; 2` and `ageph &gt;= 56`.

1. Subset the data accordingly.

1. Calculate the expected claim frequency as `sum(nclaims)/sum(expo)`.

1. Compare with the {rpart} prediction of 0.08899811.
]

---

class:clear

.hi-pink[Q.1-Q.2]: subset the data and calculate the claim frequency

```r
mtpl %&gt;% 
  dplyr::filter(bm &lt; 2,
                ageph &gt;= 56) %&gt;% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo))
```

```
##   claim_freq
## 1 0.08898655
```

.hi-pink[Q.3]: the prediction and our DIY calculation .KULbginline[do not match]! &lt;br&gt;

Is this due to a rounding error? &lt;br&gt;
Or is there something spooky .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 384 512"&gt;&lt;path d="M186.1.09C81.01 3.24 0 94.92 0 200.05v263.92c0 14.26 17.23 21.39 27.31 11.31l24.92-18.53c6.66-4.95 16-3.99 21.51 2.21l42.95 48.35c6.25 6.25 16.38 6.25 22.63 0l40.72-45.85c6.37-7.17 17.56-7.17 23.92 0l40.72 45.85c6.25 6.25 16.38 6.25 22.63 0l42.95-48.35c5.51-6.2 14.85-7.17 21.51-2.21l24.92 18.53c10.08 10.08 27.31 2.94 27.31-11.31V192C384 84 294.83-3.17 186.1.09zM128 224c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm128 0c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"/&gt;&lt;/svg&gt;] going on?

---

# Unraveling the mystery of {rpart}

.pull-left[

.KULbginline[Conceptually]: no events in a leaf node lead to division by zero in the deviance!

Solution: assume .KULbginline[Gamma prior] on the mean of the Poisson in the leaf nodes: 
  + set `\(\mu = \sum y_i / \sum \textrm{expo}_i\)`
  + use coefficient of variation `\(k = \sigma / \mu\)` as .KULbginline[user input]
  + `\(k = 0\)` extreme .KULbginline[pessimism] (all leaf nodes equal)
  + `\(k = \infty\)` extreme .KULbginline[optimism] (let the data speak)
  + default in {rpart}: `\(k=1\)`.
  
The resulting leaf node prediction: 

`$$\frac{\alpha + \sum Y_i}{\beta + \sum \text{expo}_i}, \,\,\,\,\,\,\, \alpha = 1/k^2, \,\,\,\,\,\,\, \beta=\alpha / \mu.$$`
]

.pull-right[

```r
k &lt;- 1

alpha &lt;- 1/k^2

mu &lt;- mtpl %&gt;% 
  with(sum(nclaims)/sum(expo))

beta &lt;- alpha/mu

mtpl %&gt;% 
  dplyr::filter(bm &lt; 2, ageph &gt;= 56) %&gt;% 
  dplyr::summarise(prediction = 
*         (alpha + sum(nclaims))/(beta + sum(expo)))
```

```
##   prediction
## 1 0.08899811
```

More details in Section 8.2 of the [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) on Poisson regression. 


]

---

# Coefficient of variation very low

.pull-left[

```r
fit &lt;- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               cp = 0),
*            parms = list(shrink = 10^-5)
             )
```
&lt;br&gt;
Notice that .KULbginline[all] leaf nodes predict the .KULbginline[same value].
]

.pull-right[

```
## n= 163231 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 163231 89944.320 0.1393352  
##    2) bm&lt; 6.5 127672 63858.770 0.1393352  
##      4) bm&lt; 1.5 88621 41974.470 0.1393352 *
##      5) bm&gt;=1.5 39051 21884.300 0.1393352  
##       10) ageph&gt;=57.5 8463  4346.787 0.1393352 *
##       11) ageph&lt; 57.5 30588 17537.510 0.1393352 *
##    3) bm&gt;=6.5 35559 26085.560 0.1393353 *
```
]

---

# Coefficient of variation very high

.pull-left[

```r
fit &lt;- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               cp = 0),
*            parms = list(shrink = 10^5)
             )
```

```r
# Remember this number?
mtpl %&gt;% 
  dplyr::filter(bm &lt; 2, ageph &gt;= 56) %&gt;% 
  dplyr::summarise(claim_freq = 
                     sum(nclaims)/sum(expo))
```

```
##   claim_freq
## 1 0.08898655
```

]

.pull-right[

```
## n= 163231 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 163231 89944.320 0.13933520  
##    2) bm&lt; 6.5 127672 63455.290 0.11783920  
##      4) bm&lt; 1.5 88621 41252.130 0.10550180  
##        8) ageph&gt;=55.5 33646 14281.360 0.08898655 *
##        9) ageph&lt; 55.5 54975 26835.800 0.11597980 *
##      5) bm&gt;=1.5 39051 21872.010 0.14641180  
##       10) ageph&gt;=57.5 8463  4324.098 0.11962090 *
##       11) ageph&lt; 57.5 30588 17496.720 0.15409010 *
##    3) bm&gt;=6.5 35559 24843.720 0.22190600  
##      6) bm&lt; 10.5 22657 15022.440 0.19810170  
##       12) ageph&gt;=26.5 17196 10950.970 0.18567400 *
##       13) ageph&lt; 26.5 5461  4025.443 0.23683020 *
##      7) bm&gt;=10.5 12902  9678.292 0.26762210  
##       14) agec&lt; 6.5 4472  3181.980 0.23453270 *
##       15) agec&gt;=6.5 8430  6471.783 0.28656300 *
```
]

---


name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]

.right-column[
.hi-pink[Q]: Follow the .KULbginline[pruning strategy] to develop a proper frequency tree model for the MTPL data.

1. Start from an overly complex tree. Do not forget your favorite random .hi-pink[seed] upfront!

1. Inspect the cross-validation results.

1. Choose the `cp` value minimizing `xerror` for .KULbginline[pruning].

1. Visualize the pruned tree with `rpart.plot`.
]

---

class:clear

.pull-left[
.hi-pink[Q.1]: fit an overly complex tree

```r
set.seed(9753) # reproducibilty
fit &lt;- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl,
             method = 'poisson',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 2000,
*              minbucket = 1000,
               cp = 0,
*              xval = 5
               )
             )
```
]

.pull-right[
.hi-pink[Q.2]: inspect the cross-validation results

```r
plotcp(fit)
```

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-84-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: clear

.hi-pink[Q.3]: choose the `cp` value that minimizes `xerror` for .KULbginline[pruning]

```r
# Get the cross-validation results
cpt &lt;- fit$cptable

# Look for the minimal xerror
min_xerr &lt;- which.min(cpt[,'xerror'])
cpt[min_xerr,]
##           CP       nsplit    rel error       xerror         xstd 
## 1.152833e-04 2.900000e+01 9.693815e-01 9.735286e-01 4.668765e-03

# Prune the tree
fit_srt &lt;- prune(fit,
                 cp = cpt[min_xerr, 'CP'])
```

---

class: clear

.hi-pink[Q.4]: try to understand how the final model looks like. Can you make sense of it?
.center[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-86-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle
name: interpret

# Interpretation tools

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Interpreting a tree model

.pull-left[

Interpretability depends on the .KULbginline[size of the tree]
  + is easy with a .hi-pink[shallow] tree but hard with a .hi-pink[deep] tree 
  + luckily there are some .KULbginline[tools] to aid you.

.KULbginline[Feature importance]
  + identify the most .hi-pink[important] features
  + implemented in the package {vip}. 

.KULbginline[Partial dependence plot]
  + measure the .hi-pink[marginal effect] of a feature
  + implemented in the package {pdp}.

Excellent source on interpretable machine learning: [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) book by Christophe Molnar.
]

.pull-right[
&lt;img src="img/logo_vip.png" width="40%" style="display: block; margin: auto;" /&gt;
&lt;img src="img/logo_pdp.png" width="40%" style="display: block; margin: auto;" /&gt;
]

---

# Feature importance and partial dependence

.left-column[

&lt;br&gt;
&lt;br&gt;

&lt;img src = "img/molnar.png" height = "350px" /&gt;



]

.right-column[

With .KULbginline[feature importance]: 

* sum improvements in loss function over all splits on a variable `\(x_{\ell}\)`
* important variables appear high and often in a tree.

With .KULbginline[partial dependence]: 

* univariate

`$$\bar{f}_{\ell}(x_{\ell}) = \frac{1}{n} \sum_{i=1}^n f_{\text{tree}}(x_{\ell},\boldsymbol{x}^{i}_{-\ell})$$`

* bivariate

`$$\bar{f}_{k, \ell}(x_k, x_{\ell}) = \frac{1}{n} \sum_{i=1}^n f_{\text{tree}}(x_k, x_{\ell}, \boldsymbol{x}^{i}_{-k, \ell})$$`

* marginal effects, interactions can stay hidden!

]



---

# Feature importance &lt;img src="img/logo_vip.png" class="title-hex"&gt;

.pull-left[

```r
library(vip)
# Function vi gives you the data
*var_imp &lt;- vip::vi(fit_srt)
```

```
##   Variable   Importance
## 1       bm 2186.5664729
## 2    ageph  651.1768792
## 3    power  164.2502735
## 4     fuel   70.5542003
## 5     agec   45.8086193
## 6 coverage   24.9202279
## 7      use    2.2088357
## 8      sex    0.7547625
## 9    fleet    0.2642045
```


```r
# Function vip makes the plot
*vip::vip(fit_srt, scale = TRUE)
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-92-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Partial dependence plot &lt;img src="img/logo_pdp.png" class="title-hex"&gt;

.pull-left[

```r
library(pdp)
# Need to define this helper function for Poisson
pred.fun &lt;- function(object,newdata){
  mean(predict(object, newdata))
} 

# Sample 5000 observations to speed up pdp generation
set.seed(48927)
pdp_ids &lt;- mtpl %&gt;% nrow %&gt;% sample(size = 5000)
```

```r
# partial: computes the marginal effect
# autoplot: creates the graph using ggplot2
fit_srt %&gt;% 
* pdp::partial(pred.var = 'ageph',
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %&gt;% 
* autoplot()
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-95-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Partial dependence plot in two dimensions &lt;img src="img/logo_pdp.png" class="title-hex"&gt;

.pull-left[

```r
# partial: computes the marginal effect
# autoplot: creates the graph using ggplot2
fit_srt %&gt;% 
* pdp::partial(pred.var = c('ageph','power'),
               pred.fun = pred.fun,
               train = mtpl[pdp_ids,]) %&gt;% 
  autoplot()
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-97-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]

.right-column[
Use partial dependence plots for .hi-pink[other features] to gain .KULbginline[understanding] of your model.
]

---

class:clear

.pull-left[
.KULbginline[Level in the bonus-malus scale] &lt;br&gt;
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-98-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.KULbginline[Type of coverage] &lt;br&gt;
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-99-1.png" style="display: block; margin: auto;" /&gt;
]

---

# That's a wrap on single trees!

.pull-left[

.font140[.KULbginline[Advantages]] &amp;nbsp; &amp;nbsp; .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 448c-110.3 0-200-89.7-200-200S137.7 56 248 56s200 89.7 200 200-89.7 200-200 200zm117.8-146.4c-10.2-8.5-25.3-7.1-33.8 3.1-20.8 25-51.5 39.4-84 39.4s-63.2-14.3-84-39.4c-8.5-10.2-23.7-11.5-33.8-3.1-10.2 8.5-11.5 23.6-3.1 33.8 30 36 74.1 56.6 120.9 56.6s90.9-20.6 120.9-56.6c8.5-10.2 7.1-25.3-3.1-33.8zM168 240c17.7 0 32-14.3 32-32s-14.3-32-32-32-32 14.3-32 32 14.3 32 32 32zm160-60c-25.7 0-55.9 16.9-59.9 42.1-1.7 11.2 11.5 18.2 19.8 10.8l9.5-8.5c14.8-13.2 46.2-13.2 61 0l9.5 8.5c8.5 7.4 21.6.3 19.8-10.8-3.8-25.2-34-42.1-59.7-42.1z"/&gt;&lt;/svg&gt;]

* Shallow tree is easy to .KULbginline[explain] graphically.

* Closely mirror the human .KULbginline[decision-making] process.

* Handle all types of features .KULbginline[without] pre-processing.

* .KULbginline[Fast] and very scalable to big data.

* .KULbginline[Automatic] variable selection.

* Surrogate splits can handle .KULbginline[missing] data.
]

---

# That's a wrap on single trees!

.pull-left[

.font140[.KULbginline[Advantages]] &amp;nbsp; &amp;nbsp; .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 448c-110.3 0-200-89.7-200-200S137.7 56 248 56s200 89.7 200 200-89.7 200-200 200zm117.8-146.4c-10.2-8.5-25.3-7.1-33.8 3.1-20.8 25-51.5 39.4-84 39.4s-63.2-14.3-84-39.4c-8.5-10.2-23.7-11.5-33.8-3.1-10.2 8.5-11.5 23.6-3.1 33.8 30 36 74.1 56.6 120.9 56.6s90.9-20.6 120.9-56.6c8.5-10.2 7.1-25.3-3.1-33.8zM168 240c17.7 0 32-14.3 32-32s-14.3-32-32-32-32 14.3-32 32 14.3 32 32 32zm160-60c-25.7 0-55.9 16.9-59.9 42.1-1.7 11.2 11.5 18.2 19.8 10.8l9.5-8.5c14.8-13.2 46.2-13.2 61 0l9.5 8.5c8.5 7.4 21.6.3 19.8-10.8-3.8-25.2-34-42.1-59.7-42.1z"/&gt;&lt;/svg&gt;]

* Shallow tree is easy to .KULbginline[explain] graphically.

* Closely mirror the human .KULbginline[decision-making] process.

* Handle all types of features .KULbginline[without] pre-processing.

* .KULbginline[Fast] and very scalable to big data.

* .KULbginline[Automatic] variable selection.

* Surrogate splits can handle .KULbginline[missing] data.
]

.pull-right[

.font140[.KULbginline[Disadvantages]] &amp;nbsp; &amp;nbsp; .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 448c-110.3 0-200-89.7-200-200S137.7 56 248 56s200 89.7 200 200-89.7 200-200 200zm-80-216c17.7 0 32-14.3 32-32s-14.3-32-32-32-32 14.3-32 32 14.3 32 32 32zm160-64c-17.7 0-32 14.3-32 32s14.3 32 32 32 32-14.3 32-32-14.3-32-32-32zm-80 128c-40.2 0-78 17.7-103.8 48.6-8.5 10.2-7.1 25.3 3.1 33.8 10.2 8.4 25.3 7.1 33.8-3.1 16.6-19.9 41-31.4 66.9-31.4s50.3 11.4 66.9 31.4c8.1 9.7 23.1 11.9 33.8 3.1 10.2-8.5 11.5-23.6 3.1-33.8C326 321.7 288.2 304 248 304z"/&gt;&lt;/svg&gt;]

* Tree uses .KULbginline[step] functions to approximate the effect.

* Greedy heuristic approach chooses .KULbginline[locally] optimal split (i.e., based on all previous splits).

* Data becomes .KULbginline[smaller] and smaller down the tree.

* All this results in .KULbginline[high variance] for a tree model...

* ... which harms .KULbginline[predictive performance].
]

---

class: inverse, center, middle
name: tree

# From a single tree to ensembles of trees

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Ensembles of trees

Remember: prediction error = bias + variance.

--

Good .KULbginline[predictive performance] requires low bias .KULbginline[AND] low variance.

--

Two popular .hi-pink[ensemble] algorithms (that can be applied to any type of model, not just trees) are:

--

* .KULbginline[bagging]:
  + low .hi-pink[bias] via detailed individual models (think: deep trees)
  + low .hi-pink[variance] via averaging of those models
  + .KULbginline[random forest] is a modification on bagging for trees to further improve the variance reduction.

* .KULbginline[boosting]:
  + low .hi-pink[variance] via simple individual models
  + low .hi-pink[bias] by incrementing the model sequentially.
  

---

# From our own experience

.center[
.KULbginline[Boosting &gt; Random forest &gt; Bagging &gt; Decision tree]
]

&lt;img src="img/oos_freq_poiss.png" width="50%" style="display: block; margin: auto;" /&gt;

Detailed discussion in our North American Actuarial Journal (2020) paper on .KULbginline[Boosting insights in insurance tariff plans with tree-based machine learning methods]. See [Henckaerts et al. (2020)](https://arxiv.org/abs/1904.10890).

---


class: inverse, center, middle
name: bag

# Introducing bagging

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

class: clear
background-image: url(img/bagging.jpg)
background-size: contain

---


# Bagging

.pull-left[

.KULbginline[Bagging] is for .KULbginline[B]ootstrap .KULbginline[AGG]regat.KULbginline[ING].

Simple idea:
  + build a lot of different .KULbginline[base learners] on bootstrapped samples of the data
  + .KULbginline[combine] their predictions.

Model .KULbginline[averaging] helps to:
  + reduce variance
  + avoid overfitting.

Bagging works best for .KULbginline[base learners] with:
  + .hi-pink[low bias] and .hi-pink[high variance]
  + for example: deep decison trees.
]

---

# Bagging

.pull-left[

.KULbginline[Bagging] is for .KULbginline[B]ootstrap .KULbginline[AGG]regat.KULbginline[ING].

Simple idea:
  + build a lot of different .KULbginline[base learners] on bootstrapped samples of the data
  + .KULbginline[combine] their predictions.

Model .KULbginline[averaging] helps to:
  + reduce variance
  + avoid overfitting.

Bagging works best for .KULbginline[base learners] with:
  + .hi-pink[low bias] and .hi-pink[high variance]
  + for example: deep decison trees.
]

.pull-right[

.KULbginline[Bagging with trees?]

* Do .KULbginline[B] times:
  + create .hi-pink[bootstrap sample] by drawing with replacement from the original data
  + fit a .hi-pink[deep tree] to the bootstrap sample.
  
* .KULbginline[Combine] the predictions of the B trees
  + .hi-pink[average] prediction for regression
  + .hi-pink[majorty] vote for classification.
  
This is implemented in the {ipred} package, using {rpart} under the hood.
]

---

# Bootstrap samples

.pull-left[

```r
# Set a seed for reproducibility
set.seed(45678)

# Generate the first bootstrapped sample
bsample_1 &lt;- dfr %&gt;% nrow %&gt;% 
* sample(replace = TRUE)
# Generate another bootstrapped sample
bsample_2 &lt;- dfr %&gt;% nrow %&gt;% 
* sample(replace = TRUE)

# Use the indices to sample the data
dfr_b1 &lt;- dfr %&gt;%
* dplyr::slice(bsample_1)
dfr_b2 &lt;- dfr %&gt;% 
* dplyr::slice(bsample_2)
```

```r
# Let's have a look at the sampled data
*dfr_b1 %&gt;% dplyr::arrange(x) %&gt;% head()
*dfr_b2 %&gt;% dplyr::arrange(x) %&gt;% head()
```
]

.pull-right[
Sample 1:

```
##            x          m          y
## 1 0.02518311 0.05036089 -0.7336728
## 2 0.02518311 0.05036089 -0.7336728
## 3 0.03777466 0.07553136 -1.5750691
## 4 0.06295777 0.12583237 -0.9696970
## 5 0.10073243 0.20112432  1.5964765
## 6 0.11332398 0.22616316  0.4061405
```
Sample 2:

```
##            x          m          y
## 1 0.00000000 0.00000000 -0.1789007
## 2 0.00000000 0.00000000 -0.1789007
## 3 0.00000000 0.00000000 -0.1789007
## 4 0.01259155 0.02518244 -0.9028617
## 5 0.06295777 0.12583237 -0.9696970
## 6 0.07554932 0.15095495 -1.5412872
```
]

---

# Decision tree on sample 1

.pull-left[

```r
fit_b1 &lt;- rpart(formula = y ~ x,
*            data = dfr_b1,
             method = 'anova',
             control = rpart.control(
*              maxdepth = 20,
*              minsplit = 10,
*              minbucket = 5,
*              cp = 0
               )
             )
```

&lt;br&gt;

On it's own, this is a .KULbginline[noisy prediction] with very .KULbginline[high variance]!
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-106-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Decision tree on sample 2

.pull-left[

```r
fit_b2 &lt;- rpart(formula = y ~ x,
*            data = dfr_b2,
             method = 'anova',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
               cp = 0
               )
             )
```

&lt;br&gt;

Again, very .KULbginline[high variance] on it's own!
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-108-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Combining the predictions of both trees

.pull-left[

```r
# Predictions for the first tree
pred_b1 &lt;- fit_b1 %&gt;% predict(dfr)
# Predictions for the first tree
pred_b2 &lt;- fit_b2 %&gt;% predict(dfr)

# Average the predictions
pred &lt;- rowMeans(cbind(pred_b1,
                       pred_b2))
```

&lt;br&gt;

Does it look like the prediction it's getting .KULbginline[less noisy]? 
&lt;br&gt; &lt;br&gt;
In other words: .KULbginline[is variance reducing?]
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-110-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]

.right-column[
.hi-pink[Q]: add a .KULbginline[third tree] to the .KULbginline[bagged ensemble] and inspect the predictions.

1. Generate a .KULbginline[bootstrap sample] of the data (note: don't use the same seed as before because your bootstrap samples will be the same).

1. Fit a .KULbginline[deep tree] to this bootstrap sample.

1. Make predictions for this tree and .KULbginline[average] with the others.
]

---

class: clear

.pull-left[
.hi-pink[Q1]: bootstrap sample with different seed

```r
# Generate the third bootstrapped sample
set.seed(28726)
bsample_3 &lt;- dfr %&gt;% nrow %&gt;% 
                sample(replace = TRUE)
# Use the indices to sample the data
dfr_b3 &lt;- dfr %&gt;% dplyr::slice(bsample_3)
```
.hi-pink[Q2]: fit a deep tree

```r
# Fit an unpruned tree
fit_b3 &lt;- rpart(formula = y ~ x,
             data = dfr_b3,
             method = 'anova',
             control = rpart.control(
               maxdepth = 20,
               minsplit = 10,
               minbucket = 5,
               cp = 0))
```
]

.pull-right[
.hi-pink[Q3]: average the predictions

```r
# Predictions for the third tree
pred_b3 &lt;- fit_b3 %&gt;% predict(dfr)
# Average the predictions
pred_new &lt;- rowMeans(cbind(pred_b1,
                           pred_b2,
                           pred_b3))
```
]

---

class: clear

.pull-left[
Bagged ensemble with .hi-pink[B = 2]
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-114-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[
Bagged ensemble with .hi-pink[B = 3]
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-115-1.png" style="display: block; margin: auto;" /&gt;
]

.KULbginline[Little] variance reduction might be visible, but we clearly need .KULbginline[a lot more trees]. Let's use the {ipred} package for this!

---

# Using {ipred}


```r
bagging(formula, data, control = rpart.control(___),
        nbagg, ns, coob)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...*

* `data`: the observation data containing the response and features

* `control`: options to pass to `rpart.control` for the .KULbginline[base learners]
  
* `nbagg`: the number of bagging iterations .KULbginline[B], i.e., the number of trees in the ensemble

* `ns`: number of observations to draw for the bootstrap samples (often less than N to save computational time)

* `coob`: a logical indicating whether an .KULbginline[out-of-bag] estimate of the error rate should be computed.

---

# Out-of-bag (OOB) error

.pull-left[
Bootstrap samples are constructed .KULbginline[with] replacement.

Some observations are .KULbginlnine[not present] in a bootstrap sample: 
  + they are called the .KULbginline[out-of-bag] observations
  + use those to calculate the out-of-bag (OOB) error
  + measures .KULbginline[hold-out] error like cross-validation.

Advantage of OOB over cross-validation?
  + the OOB error comes .KULbginline[for free] with bagging.
]

---


# Out-of-bag (OOB) error

.pull-left[
Bootstrap samples are constructed .KULbginline[with] replacement.

Some observations are .KULbginlnine[not present] in a bootstrap sample: 
  + they are called the .KULbginline[out-of-bag] observations
  + use those to calculate the out-of-bag (OOB) error
  + measures .KULbginline[hold-out] error like cross-validation.

Advantage of OOB over cross-validation?
  + the OOB error comes .KULbginline[for free] with bagging.
]
]

.pull-right[
But, is this a .KULbginline[representative] sample?

```r
set.seed(12345)
N &lt;- 100000 ; x &lt;- 1:N
mean(x %in% sample(N,
                   replace = TRUE))
## [1] 0.63349
```

Roughly .KULbginline[37%] of observations are OOB when N is large.

Even more when we sample .KULbginline[&lt; N] observations

```r
mean(x %in% sample(N,
*                  size = 0.75*N,
                   replace = TRUE))
## [1] 0.52837
```
]

---

# Bagging properly

.pull-left[

```r
library(ipred)
set.seed(83946) # reproducibility

# Fit a bagged tree model
fit &lt;- ipred::bagging(formula = y ~ x,
               data = dfr,
*              nbagg = 200,
*              ns = nrow(dfr),
*              coob = TRUE,
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 40,
                 minbucket = 20,
*                cp = 0)
               )
```

```r
# Predict from this model
pred &lt;- predict(fit, dfr)
```

With 200 trees we can see the .KULbginline[variance reduction]! 
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-121-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Evolution of the OOB error

.pull-left[

```r
set.seed(98765) # reproducibility
# Define a grid for B
*nbags &lt;- 10*(1:20)
*oob &lt;- rep(0, length(nbags))
# Fit a bagged tree model
for(i in 1:length(nbags)){
  fit &lt;- ipred::bagging(formula = y ~ x,
               data = dfr,
*              nbagg = nbags[i],
               ns = nrow(dfr),
*              coob = TRUE,
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 40,
                 minbucket = 20,
                 cp = 0)
               )
* oob[i] &lt;- fit$err
}
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-123-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
Use {ipred} to fit a .KULbginline[bagged] tree ensemble for the toy .hi-pink[classification] problem with data `dfc`. &lt;br&gt; &lt;br&gt;
.hi-pink[Q]: experiment with the `nbagg` and `control` parameters to see their effect on the predictions. &lt;br&gt;

]

---

class: clear

.hi-pink[Q]: the following parameter settings seem to produce a decent fit.

.pull-left[

```r
set.seed(98765) # reproducibility

# Fit a bagged tree model
fit &lt;- ipred::bagging(formula = y ~ x1 + x2,
*              data = dfc,
*              nbagg = 100,
*              ns = nrow(dfc),
               control = rpart.control(
                 maxdepth = 20,
                 minsplit = 10,
                 minbucket = 5,
                 cp = 0)
               )
```

```r
# Predict from this model
pred &lt;- predict(fit,
                newdata = dfc,
*               type = 'class',
*               aggregation = 'majority'
                )
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-126-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle
name: bag

# From bagging to random forests

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Problem of dominant features

A downside of bagging is that .KULbginline[dominant features] can cause individual trees to have a .KULbginline[similar structure]
  
  + known as .KULbginline[tree correlation]

--

Remember the .hi-pink[feature importance] results discussed earlier for the MTPL data? 
  
  + `bm` is a very dominant variable
  + `ageph` was rather important
  + `power` also, but to a lesser degree.

--

Problem?
  
  + bagging gets its predictive performance from .hi-pink[variance reduction]
  + however, this reduction .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] when tree correlation .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;]
  + dominant features therefore .hi-pink[hurt] the preditive performance of a bagged ensemble!

--

Solution?
  
  + .KULbginline[Random forest].
  
---

# Random forest

.KULbginline[Random forest] is a modification on bagging to get an ensemble of .hi-pink[de-correlated] trees.

--

Process is very similar to bagging, with one small .KULbginline[trick]:
  + before each split, select a .hi-pink[subset of features] at random as candidate features for splitting
  + this essentially .KUlbginline[decorrelates] the trees in the ensemble, improving predictive performance
  + the number of candidates is typically considered a .KUlbginline[tuning parameter]

--

.KULbginline[Bagging] introduces randomness in the .hi-pink[rows] of the data.

--

.KULbginline[Random forest] introduces randomness in the .KULbginline[rows] and .KULbginline[columns] of the data.

--

Many .hi-pink[packages] available, but a couple of popular ones: 
  + {randomForest}: standard for regression and classification, but not very fast
  + {randomForestSRC}: fast OpenMP implementation for survival, regression and classification
  + {ranger}: fast C++ implementation for survival, regression and classification.

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
.hi-pink[Q]: let's investigate the issue of dominant features.

1. Take .KULbginline[two bootstrap samples] from the .KULbginline[MTPL] data.

1. Fit a regression tree of .KULbginline[depth = 3] to each sample.

1. Check the resulting tree structures.
]

---

class:clear

.pull-left[
.hi-pink[Q1]: two bootstrap samples


```r
set.seed(486291) # reproducibility

# Generate the first bootstrapped sample
bsample_1 &lt;- mtpl %&gt;% nrow %&gt;% 
                sample(replace = TRUE)

# Generate another bootstrapped sample
bsample_2 &lt;- mtpl %&gt;% nrow %&gt;% 
                sample(replace = TRUE)

# Use the indices to sample the data
mtpl_b1 &lt;- mtpl %&gt;% dplyr::slice(bsample_1)
mtpl_b2 &lt;- mtpl %&gt;% dplyr::slice(bsample_2)
```
]

.pull-right[
.hi-pink[Q2]: Poisson regression tree for each sample

```r
fit_b1 &lt;- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl_b1,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               minsplit = 2000,
               minbucket = 1000,
               cp = 0))

fit_b2 &lt;- rpart(formula = 
               cbind(expo,nclaims) ~
               ageph + agec + bm + power + 
               coverage + fuel + sex + fleet + use,
             data = mtpl_b2,
             method = 'poisson',
             control = rpart.control(
               maxdepth = 3,
               minsplit = 2000,
               minbucket = 1000,
               cp = 0))
```
]

---

class:clear

.hi-pink[Q3]: the resulting tree structures
.pull-left[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-129-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-130-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Using {ranger}


```r
ranger(formula, data, num.trees, mtry, min.node.size, max.depth,
       replace, sample.fraction, oob.error, num.threads, seed)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...*

* `data`: the observation data containing the response and features

* `num.trees`: the number of .hi-pink[trees] in the ensemble

* `mtry`: the number of .hi-pink[candidate] features for splitting

* `min.node.size` and `max.depth`: minimal leaf node size and maximal depth for the individual trees

*  `replace` and `sample.fraction`: sample with/without replacement and fraction of observations to sample

* `oob.error`: boolean indication to calculate the .hi-pink[OOB] error

* `num.threads` and `seed`: number of threads and random seed.

---

# Tuning strategy for random forests

.pull-left[
Many .KULbginline[tuning] parameters in a .hi-pink[random forest]:
  + number of trees
  + number of candidates for splitting
  + max tree depth
  + minimun leaf node size
  + sample fraction.

Construct a full .KULbginline[Cartesian] grid via `expand.grid`:


```r
search_grid &lt;- expand.grid(
  num.trees = c(100,200),
  mtry = c(3,6,9),
  min.node.size = c(0.001,0.01)*nrow(mtpl),
  error = NA
  )
```

]

.pull-right[

```r
print(search_grid)
##    num.trees mtry min.node.size error
## 1        100    3       163.231    NA
## 2        200    3       163.231    NA
## 3        100    6       163.231    NA
## 4        200    6       163.231    NA
## 5        100    9       163.231    NA
## 6        200    9       163.231    NA
## 7        100    3      1632.310    NA
## 8        200    3      1632.310    NA
## 9        100    6      1632.310    NA
## 10       200    6      1632.310    NA
## 11       100    9      1632.310    NA
## 12       200    9      1632.310    NA
```
]

---

# Tuning strategy for random forests (cont.)

.pull-left[
Perform a .KULbginline[grid search] and track the .KULbginline[OOB error]:

```r
library(ranger)
for(i in seq_len(nrow(search_grid))) {
  # fit a random forest for the ith combination
  fit &lt;- ranger(
    formula = nclaims ~
              ageph + agec + bm + power + 
              coverage + fuel + sex + fleet + use, 
    data = mtpl, 
*   num.trees = search_grid$num.trees[i],
*   mtry = search_grid$mtry[i],
*   min.node.size = search_grid$min.node.size[i],
    replace = TRUE,
    sample.fraction = 0.75,
    verbose = FALSE,
    seed = 54321
  )
  # get the OOB error 
* search_grid$error[i] &lt;- fit$prediction.error
}
```
]

.pull-right[

```r
search_grid %&gt;% arrange(error)
##    num.trees mtry min.node.size     error
## 1        200    3      1632.310 0.1332844
## 2        100    3      1632.310 0.1333003
## 3        200    6      1632.310 0.1333551
## 4        200    9      1632.310 0.1333689
## 5        100    6      1632.310 0.1333754
## 6        100    9      1632.310 0.1333862
## 7        200    3       163.231 0.1337361
## 8        100    3       163.231 0.1338148
## 9        200    6       163.231 0.1341431
## 10       100    6       163.231 0.1342326
## 11       200    9       163.231 0.1343189
## 12       100    9       163.231 0.1344154
```

What does the prediction error .KULbginline[measure] actually? &lt;br&gt;
The .hi-pink[Mean Squared Error], but does that make sense for us?
]

---

# Random forests for actuaries &lt;img src="img/distRforest.png" class="title-hex"&gt;
  
All available random forest packages only support .KULbginline[standard regression] based on the .hi-pink[Mean Squared Error]
  
  + no Poisson, Gamma or log-normal loss functions available
  + bad news for actuaries.

--
  
Possible solution: the {distRforest} package on Roel's [GitHub]([https://github.com/henckr/distRforest) .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt;]
  
  + based on {rpart} which supports .KULbginline[Poisson] regression (as we have seen before)
  + extended to support .KULbginline[Gamma] and .KULbginline[log-normal] deviance as loss function
  + extended to support .KULbginline[random forest] generation
  + this package is used in [Henckaerts et al. (2020)](https://arxiv.org/abs/1904.10890).

---

class: inverse, center, middle
name: bag

# (Stochastic) Gradient Boosting Machines

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

class: clear
background-image: url(img/boosting.jpg)
background-size: contain

---

# Boosting vs. bagging

Similar to bagging, boosting is a .KULbginline[general technique] to create an .KULbginline[ensemble] of any type of base learner.

--

However, there are some .KULbginline[key differences] between both approaches:

.pull-left[

With .KULbginline[bagging]:

* .KULbginline[Strong base learners]
  + low bias, high variance
  + for example: deep trees
* .KULbginline[Variance reduction] in ensemble through .KULbginline[averaging]
* .KULbginline[Parallel] approach
  + trees not using information from each other
  + performance thanks to .hi-pink[averaging]
  + low risk for overfitting.
]

.pull-right[

With .KULbginline[boosting]:

* .KULbginline[Weak base learners]
  + low variance, high bias
  + for example: stumps
* .KULbginline[Bias reduction] in ensemble through .KULbginline[updates]
* .KULbginline[Sequential] approach
  + current tree uses information from all past trees
  + performance thanks to .hi-pink[rectifying] past mistakes
  + high risk for overfitting.
]

---

# GBM: stochastic gradient boosting with trees

.left-column[
We focus on .KULbginline[GBM]:
  + stochastic gradient boosting with .KULbginline[decision trees]
  + stochastic: .KULbginline[subsampling] in the rows (and columns) of the data
  + gradient: optimizing the loss function via .KULbginline[gradient descent].
]

.right-column[

&lt;img src="img/gradient_descent.png" width="85%" style="display: block; margin: auto;" /&gt;
&lt;br&gt; Figure 12.3 from Boehmke &amp; Greenwell [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/gbm.html).
]

---

# Stochastic gradient descent

The .KULbginline[learning rate] (also called step size) is very important in gradient descent
  + too big: likely to .hi-pink[overshoot] the optimal solution
  + too small: .hi-pink[slow] process to reach the optimal solution
  
.center[
&lt;img src="img/learning_rate.png" width="80%" style="display: block; margin: auto;" /&gt;
&lt;br&gt; Figure 12.4 from Boehmke &amp; Greenwell [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/gbm.html).
]

--

.KULbginline[Subsampling] allows to escape plateaus or local minima for .hi-pink[non-convex] loss functions.

---

# GBM training process

.KULbginline[Initialize] the model fit with a global average and calculate .hi-pink[pseudo-residuals].

--

Do the following .KULbginline[B] times:
  + fit a tree of a pre-specified depth to the .KULbginline[pseudo-residuals]
  + .KULbginline[update] the model fit and pseudo-residuals with a .KULbginline[shrunken] version 
  + shrinkage to slow down learning and .KULbginline[prevent] overfitting.

--

The model fit after B iterations is the .KULbginline[end product].

--

Some .KULbginline[popular] packages for stochastic gradient boosting 
  + {gbm}: standard for regression and classification, but not the fastest
  + {gbm3}: faster version of {gbm} via parallel processing, but not backwards compatible
  + {xgboost}: efficient implementation with some .hi-pink[extra] elements, for example regularization.

---

# Using {gbm}


```r
gbm(formula, data, distribution, var.monotone, n.trees,
    interaction.depth, shrinkage, n.minobsinnode, bag.fraction, cv.folds)
```

* `formula`: a formula as *response ~ feature1 + feature2 + ...* &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; .font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] can contain an .hi-pink[offset]!

* `data`: the observation data containing the response and features

* `distribution`: a string specifying which .hi-pink[loss function] to use (gaussian, laplace, tdist, bernoulli, poisson, coxph,...)

* `var.monotone`: vector indicating a monotone increasing (+1), decreasing (-1), or arbitrary (0) relationship

* `n.trees`: the number of .hi-pink[trees] in the ensemble

* `interaction.depth` and `n.minobsinnode`: the maximum tree .hi-pink[depth] and minimum number of leaf node observations

* `shrinkage`: shrinkage parameter applied to each tree in the expansion (also called: .hi-pink[learning rate] or step size)
  
* `bag.fraction`: fraction of observations to sample for building the next tree

* `cv.folds`: number of cross-validation folds to perform.

---

# GBM parameters

A lot of parameters at our disposal to .KULbginline[tweak] the GBM.

--

Some have a .KULbginline[big impact] on the performance and should therefore be .KULbginline[properly tuned]
  + `n.trees`: depends very much on the .hi-pink[use case], ranging from 100's to 10 000's
  + `interaction.depth`: .hi-pink[low] values are preferred for boosting to obtain weak base learners
  + `shrinkage`: typically set to the lowest possible value that is .hi-pink[computationally] feasible.

--

.hi-pink[Rule of thumb]: if `shrinkage` .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"/&gt;&lt;/svg&gt;] then `ntrees` .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M8 256C8 119 119 8 256 8s248 111 248 248-111 248-248 248S8 393 8 256zm143.6 28.9l72.4-75.5V392c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24V209.4l72.4 75.5c9.3 9.7 24.8 9.9 34.3.4l10.9-11c9.4-9.4 9.4-24.6 0-33.9L273 107.7c-9.4-9.4-24.6-9.4-33.9 0L106.3 240.4c-9.4 9.4-9.4 24.6 0 33.9l10.9 11c9.6 9.5 25.1 9.3 34.4-.4z"/&gt;&lt;/svg&gt;].

--

Let's have a look at the .KULbginline[impact] of these tuning parameters.

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[stumps], .hi-pink[without] applying shrinkage:

```r
library(gbm)
# Fit the GBM
fit &lt;- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
*          n.trees = 10,
*          interaction.depth = 1,
*          shrinkage = 1
           ) 

# Predict from the GBM
pred &lt;- predict(fit,
*               n.trees = fit$n.trees,
                type = 'response')
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-140-1.gif" style="display: block; margin: auto;" /&gt;
]

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[stumps], .hi-pink[with] shrinkage:

```r
# Fit the GBM
fit &lt;- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
           interaction.depth = 1,
*          shrinkage = 0.1
           ) 
```

Applying shrinkage .KULbginline[slows down] the learning process: &lt;br&gt;

* .hi-pink[avoids] overfitting &lt;br&gt;
* but we need more trees and .hi-pink[longer] training time.
]



.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-142-1.gif" style="display: block; margin: auto;" /&gt;
]


---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[shallow] trees, .hi-pink[with] shrinkage:

```r
# Fit the GBM
fit &lt;- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
*          interaction.depth = 3,
           shrinkage = 0.1
           ) 
```

Increasing tree .KULbginline[depth] allows more versatile splits: &lt;br&gt;

* .hi-pink[faster] learning &lt;br&gt;
* risk of .hi-pink[overfitting] (shrinkage important!) &lt;br&gt;

.font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] `interaction.depth &gt; 1` allows for .KULbginline[interactions]!
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-144-1.gif" style="display: block; margin: auto;" /&gt;
]

---

# GBM parameters (cont.)

.pull-left[
Fit a GBM of 10 .KULbginline[shallow] trees, .hi-pink[without] shrinkage:

```r
# Fit the GBM
fit &lt;- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
           n.trees = 10,
           interaction.depth = 3,
*          shrinkage = 1
           ) 
```

The .hi-pink[danger] for overfitting is real! &lt;br&gt;

.KULbginline[Rule of thumb]: 

* set `shrinkage &lt;= 0.01` and adjust `n.trees` accordingly (.hi-pink[computational constraint]).
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-146-1.gif" style="display: block; margin: auto;" /&gt;
]

---

# Adding trees to the ensemble

.pull-left[
Fit a GBM of .KULbginline[300] shallow trees, with shrinkage:

```r
# Fit the GBM
fit &lt;- gbm(formula = y ~ x,
           data = dfr,
           distribution = 'gaussian',
*          n.trees = 300,
           interaction.depth = 3,
           shrinkage = 0.01
           ) 
```

.font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zM112 223.4c3.3-42.1 32.2-71.4 56-71.4s52.7 29.3 56 71.4c.7 8.6-10.8 11.9-14.9 4.5l-9.5-17c-7.7-13.7-19.2-21.6-31.5-21.6s-23.8 7.9-31.5 21.6l-9.5 17c-4.3 7.4-15.8 4-15.1-4.5zm250.8 122.8C334.3 380.4 292.5 400 248 400s-86.3-19.6-114.8-53.8c-13.5-16.3 11-36.7 24.6-20.5 22.4 26.9 55.2 42.2 90.2 42.2s67.8-15.4 90.2-42.2c13.6-16.2 38.1 4.3 24.6 20.5zm6.2-118.3l-9.5-17c-7.7-13.7-19.2-21.6-31.5-21.6s-23.8 7.9-31.5 21.6l-9.5 17c-4.1 7.3-15.6 4-14.9-4.5 3.3-42.1 32.2-71.4 56-71.4s52.7 29.3 56 71.4c.6 8.6-11 11.9-15.1 4.5z"/&gt;&lt;/svg&gt;] Look at that nice fit! &lt;br&gt;

.font200[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 576 512"&gt;&lt;path d="M576 240c0-23.63-12.95-44.04-32-55.12V32.01C544 23.26 537.02 0 512 0c-7.12 0-14.19 2.38-19.98 7.02l-85.03 68.03C364.28 109.19 310.66 128 256 128H64c-35.35 0-64 28.65-64 64v96c0 35.35 28.65 64 64 64h33.7c-1.39 10.48-2.18 21.14-2.18 32 0 39.77 9.26 77.35 25.56 110.94 5.19 10.69 16.52 17.06 28.4 17.06h74.28c26.05 0 41.69-29.84 25.9-50.56-16.4-21.52-26.15-48.36-26.15-77.44 0-11.11 1.62-21.79 4.41-32H256c54.66 0 108.28 18.81 150.98 52.95l85.03 68.03a32.023 32.023 0 0 0 19.98 7.02c24.92 0 32-22.78 32-32V295.13C563.05 284.04 576 263.63 576 240zm-96 141.42l-33.05-26.44C392.95 311.78 325.12 288 256 288v-96c69.12 0 136.95-23.78 190.95-66.98L480 98.58v282.84z"/&gt;&lt;/svg&gt;] Always .KULbginline[beware] of .hi-pink[overfitting] when adding trees!
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-148-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
.hi-pink[Q]: use the previous code to .hi-pink[experiment] with your .KULbginline[GBM parameters] of choice (see `?gbm`).
]

---

class:clear

.pull-left[
Monotonic increasing fit via `var.monotone = 1`:

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-149-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
Monotonic decreasing fit via `var.monotone = -1`:

&lt;img src="ML_part2_files/figure-html/unnamed-chunk-150-1.png" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle
name: bag

# Tuning

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Classification with {gbm}

Let's .KULbginline[experiment] with the classification example (`data = dfc`) to get a grip on the tuning of .hi-pink[GBM parameters]

--

Which `distribution` to specify for .KULbginline[classification]? &lt;br&gt;
Possible .hi-pink[candidates] are:
* `"bernoulli"`: logistic regression for 0-1 outcomes
* `"huberized"`: huberized hinge loss for 0-1 outcomes
* `"adaboost"`: the AdaBoost exponential loss for 0-1 outcomes 

--

.KULbginline[Watch out]: gbm does not take factors as response so you need to .hi-pink[recode y]
  + either to a .hi-pink[numeric] in the range [0,1]
  + or a .hi-pink[boolean] `TRUE`/`FALSE`


```r
dfc &lt;- dfc %&gt;% dplyr::mutate(y_recode = as.integer(as.character(y)))
```

---

# Different parameter combinations

Set up a grid for the parameters and list to save results:

```r
ctrl_grid &lt;- expand.grid(depth = c(1,3,5),
                         shrinkage = c(0.01,0.1,1))
results &lt;- vector('list', length = nrow(ctrl_grid))
```

--

Fit different a GBM with 100 trees for each parameter combination:

```r
for(i in seq_len(nrow(ctrl_grid))) {
  fit &lt;- gbm(y_recode ~ x1 + x2,
*            data = dfc,
*            distribution = 'bernoulli',
*            n.trees = 100,
*            interaction.depth = ctrl_grid$depth[i],
*            shrinkage = ctrl_grid$shrinkage[i])
  
  # Save predictions, both the probabilities and the class
  results[[i]] &lt;- dfc %&gt;% mutate(
    depth = factor(paste('depth =',ctrl_grid$depth[i]), ordered =TRUE),
    shrinkage = factor(paste('shrinkage =',ctrl_grid$shrinkage[i]), ordered = TRUE),
*   pred_prob = predict(fit, n.trees = fit$n.trees, type = 'response'),
*   pred_clas = factor(1*(predict(fit, n.trees = fit$n.trees, type = 'response') &gt;= 0.5)))
}
```

---

# Resulting fits

.pull-left[
The predicted .hi-pink[probabilities] 
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-154-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
The predicted .hi-pink[classes]
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-155-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
.hi-pink[Q]: complete the code below to find the .hi-pink[optimal combination] of .KULbginline[tuning parameters].

1. Set up a search grid.

1. Fit a GBM for each combination op parameters.

1. Extract the OOB performance for each GBM. &lt;br&gt; .KULbginline[Beware:] a fitted `gbm` object returns the .hi-pink[improvements] in OOB error via `$oobag.improve`.


```r
my_grid &lt;- expand.grid(___)
my_grid &lt;- my_grid %&gt;% dplyr::mutate(oob_improv = NA)

for(i in seq_len(nrow(my_grid))) {
  fit &lt;- gbm(y_recode ~ x1 + x2,
             data = dfc,
             distribution = 'bernoulli',
             n.trees = ___,
             interaction.depth = ___,
             shrinkage = ___,
             ___)
  my_grid$oob_improv[i] &lt;- sum(fit$oobag.improve)
}
```

]

---

class:clear

.pull-left[
Performing the grid search:

```r
my_grid &lt;- expand.grid(depth = c(1,3,5),
                       shrinkage = c(0.01,0.1,1))
my_grid &lt;- my_grid %&gt;% dplyr::mutate(oob_improv = NA)

for(i in seq_len(nrow(my_grid))) {
  fit &lt;- gbm(y_recode ~ x1 + x2,
             data = dfc,
             distribution = 'bernoulli',
             n.trees = 100,
             interaction.depth = my_grid$depth[i],
             shrinkage = my_grid$shrinkage[i])
  my_grid$oob_improv[i] &lt;- sum(fit$oobag.improve)
}
```
]

.pull-right[
The results:

```r
my_grid %&gt;% dplyr::arrange(desc(oob_improv))
##   depth shrinkage   oob_improv
## 1     5      0.10  0.179728774
## 2     3      0.10  0.175448387
## 3     5      0.01  0.123874131
## 4     3      0.01  0.100252315
## 5     1      0.10  0.067270554
## 6     1      0.01  0.039404600
## 7     3      1.00  0.036592026
## 8     1      1.00  0.006891027
## 9     5      1.00 -0.094434251
```
]

&lt;br&gt;

Another tuning option is to set `cv.folds &gt; 0` and track the .KULbginline[cross-validation] error via `fit$cv.error`. &lt;br&gt; &lt;br&gt;
That would be a more general approach but also .hi-pink[more time-consuming].

---

class: inverse, center, middle
name: bag

# Claim frequency and severity modeling with {gbm}

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# Claim frequency modeling

.pull-left[

```r
set.seed(76539) # reproducibility
fit &lt;- gbm(formula = nclaims ~ 
              ageph + agec + bm + power + 
              coverage + fuel + sex + fleet + use + 
*             offset(log(expo)),
            data = mtpl,
*           distribution = 'poisson',
*           var.monotone = c(0,0,1,0,0,0,0,0,0),
            n.trees = 200,
            interaction.depth = 3,
            n.minobsinnode = 1000,
            shrinkage = 0.1,
*           bag.fraction = 0.75,
            cv.folds = 0
           )
```
]

.pull-right[
- Include the log of exposure as an .KULbginline[offset].

- Specify the .KULbginline[Poisson] distribution for the target.

- Impose a .KULbginline[monotonically increasing] constraint on `bm`.

- Perform .KULbginline[stochastic] gradient boosting with `bag.fraction &lt; 1`.
]
---

# Inspecting the individual trees
  

```r
fit %&gt;% 
* pretty.gbm.tree(i.tree = 1) %&gt;%
  print(digits = 4)
##   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
## 0        2      6.500000        1         5           9         164.94 122423
## 1        2      1.500000        2         3           4          27.29  95743
## 2       -1     -0.027378       -1        -1          -1           0.00  66472
## 3       -1      0.004986       -1        -1          -1           0.00  29271
## 4       -1     -0.017484       -1        -1          -1           0.00  95743
## 5        2     10.500000        6         7           8          15.42  26680
## 6       -1      0.035538       -1        -1          -1           0.00  17014
## 7       -1      0.065038       -1        -1          -1           0.00   9666
## 8       -1      0.046226       -1        -1          -1           0.00  26680
## 9       -1     -0.003599       -1        -1          -1           0.00 122423
##   Prediction
## 0  -0.003599
## 1  -0.017484
## 2  -0.027378
## 3   0.004986
## 4  -0.017484
## 5   0.046226
## 6   0.035538
## 7   0.065038
## 8   0.046226
## 9  -0.003599
```

* Does not look that .hi-pink[pretty] right?!

--

* Even if this was a nicer representation, .KULbginline[single trees] are .hi-pink[not] going to carry much information

--

* Luckily we know some .hi-pink[tools] to get a better .KULbginline[understanding] of the GBM fit!

---

# Feature importance

Applying the `summary` function on a object of class `gbm` shows built-in feature importance results:

.pull-left[

```r
summary(fit, plotit = FALSE)
##               var    rel.inf
## bm             bm 61.3816154
## ageph       ageph 15.5230257
## power       power  8.4368204
## agec         agec  6.7923989
## fuel         fuel  3.2694791
## sex           sex  2.7324191
## coverage coverage  1.3318440
## fleet       fleet  0.3382497
## use           use  0.1941476
```
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-162-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Partial dependence plot

.pull-left[
Use the following .hi-pink[helper function] for the pdps:

```r
pred.fun &lt;- function(object,newdata){
  mean(predict(object, newdata,
               n.trees = object$n.trees,
               type = 'response'))
} 
```

Partial dependence of the .hi-pink[bonus-malus level]:

```r
set.seed(48927)
pdp_ids &lt;- mtpl %&gt;% nrow %&gt;% sample(size = 5000)
fit %&gt;% 
* partial(pred.var = 'bm',
          pred.fun = pred.fun,
          train = mtpl[pdp_ids,],
          recursive= FALSE) %&gt;% 
  autoplot()
```

Notice that the monotonic constraint is satisfied.
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-165-1.png" style="display: block; margin: auto;" /&gt;
]

---

# The age effect in a single tree and a gbm

.pull-left[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-166-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-167-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Claim severity modeling

From the `gbm` help on the `distribution` argument:

&gt; Currently available options are "gaussian" (squared error), "laplace" (absolute loss), "tdist" (t-distribution loss), "bernoulli" (logistic regression for 0-1 outcomes), "huberized" (huberized hinge loss for 0-1 outcomes), "adaboost" (the AdaBoost exponential loss for 0-1 outcomes), "poisson" (count outcomes), "coxph" (right censored observations), "quantile", or "pairwise" (ranking measure using the LambdaMart algorithm)

Which to choose for .KULbginline[claim severity]?

--

Possible solution: the {gbm} version on Harry Southworth's [GitHub]([https://github.com/harrysouthworth/gbm) .font150[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt;]


```r
install.packages("devtools")
devtools::install_github("harrysouthworth/gbm")
```

---

class: inverse, center, middle
name: bag

# XGBoost

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# XGBoost

.KULbginline[XGBoost] stands for e.hi-pink[X]treme .hi-pink[G]radient .hi-pink[Boost]ing.

--

Optimized gradient boosting library: efficient, flexible and portable across multiple languages.

--

XGBoost follows the same general boosting approach as GBM, but adds some .KULbginline[extra elements]:
  + .hi-pink[regularization]: extra protection against overfitting (see Lasso and glmnet on Day 1)
  + .hi-pink[early stopping]: stop model tuning when improvement slows down
  + .hi-pink[parallel processing]: can deliver huge speed gains
  + different .hi-pink[base learners]: boosted GLMs are a possibility
  + multiple .hi-pink[languages]: implemented in R, Python, C++, Java, Scala and Julia

--

XGBoost also allows to .KULbginline[subsample columns] in the data, much like the random forest did
  + GBM only allowed subsampling of rows
  + XGBoost therefore .hi-pink[unites] boosting and random forest to some extent.

--

Very .KULbginline[flexible] method with many many parameters, full list can be found [here](https://xgboost.readthedocs.io/en/latest/parameter.html).  

---

# Using {xgboost}


```r
xgboost(data, nrounds, early_stopping_rounds, params)
```

* `data`: training data, preferably an `xgb.DMatrix` (also accepts `matrix`, `dgCMatrix`, or name of a local data file) 
* `nrounds`: max number of boosting .hi-pink[iterations]
* `early_stopping_rounds`: training with a validation set will .hi-pink[stop] if the performance doesn’t improve for k rounds
* `params`: the list of .KULbginline[parameters]
  + `booster`: gbtree, gblinear or dart
  + `objective`: reg:squarederror, binary:logistic, count:poisson, survival:cox, reg:gamma, reg:tweedie, ...
  + `eval_metric`: rmse, mae, logloss, auc, poisson-nloglik, gamma-nloglik, gamma-deviance, tweedie-nloglik, ...
  + `base_score`: initial prediction for all observations (global bias)
  + `nthread`: number of parallel threads used to run XGBoost (defaults to max available)
  + `eta`: .hi-pink[learning rate] or step size used in update to prevent overfitting
  + `gamma`: minimum loss reduction required to make a further partition on a leaf node
  + `max_depth` and `min_child_weight`: maximum depth and minimum leaf node observations
  + `subsample` and `colsample_by*`: subsample rows and columns (bytree, bylevel or bynode)
  + `lambda` and `alpha`: L2 an L1 .hi-pink[regularization] term to prevent overfitting
  + `monotone_constraints`: constraint on variable monotonicity

---

# Supplying the data to XGBoost


```r
xgb.DMatrix(data, info = list())
```

* `data`: a `matrix` object
* `info`: a named list of additional information

--


```r
library(xgboost)
mtpl_xgb &lt;- xgb.DMatrix(data = mtpl %&gt;% 
*                         select(ageph,power,bm,agec,coverage,fuel,sex,fleet,use) %&gt;%
*                         data.matrix,
                        info = list(
*                         'label' = mtpl$nclaims,
*                         'base_margin' = log(mtpl$expo)))
```
* Features go into the .hi-pink[data] argument (needs to be converted to a matrix)
* The target and offset are specified via `label` and `base_margin` in .hi-pink[info] respectively

--

This results in an xgb.DMatrix object:

```r
print(mtpl_xgb)
## xgb.DMatrix  dim: 163231 x 9  info: label base_margin  colnames: yes
```


---

# A simple XGBoost model

.pull-left[

```r
set.seed(86493) # reproducibility
fit &lt;- xgboost(
* data = mtpl_xgb,
  nrounds = 200,
* early_stopping_rounds = 20,
  verbose = FALSE,
  params = list(
*   booster = 'gbtree',
*   objective  = 'count:poisson',
*   eval_metric = 'poisson-nloglik',
    eta = 0.1, nthread = 1,
*   subsample = 0.75, colsample_bynode = 0.5,
    max_depth = 3, min_child_weight = 1000,
*   gamma = 0, lambda = 1, alpha = 1
    )
  )
```

]

.pull-right[
- Fit an XGBoost model to the .hi-pink[xgb.DMatrix] data

- Perform .hi-pink[early stopping] after 20 iterations without improvement

- Use a .hi-pink[decision tree] as base learner

- Choose the .hi-pink[Poisson] distribution for the target

- Stochastic boosting in .hi-pink[rows] and random split candidates in .hi-pink[columns] (like random forest)

- Apply .hi-pink[regularization] comparable to the elastic net penalty in {glmnet}
]

---


# Inspecting single trees
  
.pull-left[
* Possible to inspect .KULbginline[single trees] via `xgb.plot.tree`:
  + note that the trees are .hi-pink[0-indexed]
  + 0 returns first tree, 1 returns second tree,...
  + can also supply a vector of indexes
  

```r
xgb.plot.tree(
  feature_names = colnames(mtpl_xgb),
  model = fit,
  trees = 0
  )
```
]

.pull-right[
<div id="htmlwidget-397c0ab1d7c81e3df0ed" style="width:504px;height:504px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-397c0ab1d7c81e3df0ed">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       rankdir = \"LR\"]\n\nnode [color = \"DimGray\",\n      style = \"filled\",\n      fontname = \"Helvetica\"]\n\nedge [color = \"DimGray\",\n     arrowsize = \"1.5\",\n     arrowhead = \"vee\",\n     fontname = \"Helvetica\"]\n\n  \"1\" [label = \"Tree 0\nbm\nCover: 219616.938\nGain: 96.2578125\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"2\" [label = \"fuel\nCover: 167417.031\nGain: 4.484375\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"3\" [label = \"power\nCover: 52199.9062\nGain: 2.90087891\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"4\" [label = \"Leaf\nCover: 51041.7578\nValue: -0.043068964\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"5\" [label = \"power\nCover: 116375.273\nGain: 0.783203125\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"6\" [label = \"Leaf\nCover: 7697.00977\nValue: -0.0410524569\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"7\" [label = \"coverage\nCover: 44502.8984\nGain: 3.52197266\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"8\" [label = \"Leaf\nCover: 96134.4688\nValue: -0.044502791\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"9\" [label = \"Leaf\nCover: 20240.8027\nValue: -0.0434461571\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"10\" [label = \"Leaf\nCover: 16108.7969\nValue: -0.0399594866\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"11\" [label = \"Leaf\nCover: 28394.0996\nValue: -0.037880104\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n\"1\"->\"2\" [label = \"< 5.5\", style = \"bold\"] \n\"2\"->\"4\" [label = \"< 1.5\", style = \"bold\"] \n\"3\"->\"6\" [label = \"< 38.5\", style = \"bold\"] \n\"5\"->\"8\" [label = \"< 75.5\", style = \"bold\"] \n\"7\"->\"10\" [label = \"< 2.5\", style = \"bold\"] \n\"1\"->\"3\" [style = \"bold\", style = \"solid\"] \n\"2\"->\"5\" [style = \"solid\", style = \"solid\"] \n\"3\"->\"7\" [style = \"solid\", style = \"solid\"] \n\"5\"->\"9\" [style = \"solid\", style = \"solid\"] \n\"7\"->\"11\" [style = \"solid\", style = \"solid\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
]

---


# XGBoost in one tree

.pull-left[
* Get a .KULbginline[compressed view] of an XGBoost model via `xgb.plot.multi.trees`:
  + compressing an ensemble of trees into a single .hi-pink[tree-graph] representation
  + goal is to improve the interpretability


```r
xgb.plot.multi.trees(
  model = fit,
  feature_names = colnames(mtpl_xgb)
  )
```
]

.pull-right[
<div id="htmlwidget-adff0ab74f1256711043" style="width:504px;height:504px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-adff0ab74f1256711043">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       rankdir = \"LR\"]\n\nnode [color = \"DimGray\",\n      fillcolor = \"beige\",\n      style = \"filled\",\n      shape = \"rectangle\",\n      fontname = \"Helvetica\"]\n\nedge [color = \"DimGray\",\n     arrowsize = \"1.5\",\n     arrowhead = \"vee\",\n     fontname = \"Helvetica\"]\n\n  \"1\" [label = \"bm (3234.440)\ncoverage (  22.521)\nageph (1348.960)\nfuel ( 165.678)\nagec ( 114.329)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"2\" [label = \"fuel ( 69.443)\nbm (641.900)\nsex ( 36.254)\npower ( 87.941)\nageph (328.725)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"3\" [label = \"power ( 126.073)\nbm (1795.394)\nsex (  28.363)\ncoverage (  44.880)\nageph ( 295.506)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"4\" [label = \"Leaf ( -0.28526)\nageph (118.82391)\npower ( 44.16008)\nbm (169.01990)\nagec ( 45.18060)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"5\" [label = \"power ( 88.61866)\nagec ( 76.41640)\nbm (290.91521)\nfuel ( 28.64410)\nLeaf ( -0.19579)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"6\" [label = \"Leaf ( -0.13222)\nageph (182.85012)\nfuel ( 38.76425)\npower (133.47981)\nbm (331.04600)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"7\" [label = \"coverage ( 35.424)\nageph (211.107)\nbm (636.654)\nagec ( 96.537)\nfuel ( 23.013)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"8\" [label = \"Leaf (-1.7959)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"9\" [label = \"Leaf (-1.6046)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"10\" [label = \"Leaf (-1.6987)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"11\" [label = \"Leaf (-1.339)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"12\" [label = \"Leaf (-1.5715)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"13\" [label = \"Leaf (-1.559)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"14\" [label = \"Leaf (-1.7686)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"15\" [label = \"Leaf (-1.6407)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"1\"->\"2\" \n  \"2\"->\"4\" \n  \"3\"->\"6\" \n  \"5\"->\"8\" \n  \"7\"->\"10\" \n  \"4\"->\"12\" \n  \"6\"->\"14\" \n  \"1\"->\"3\" \n  \"2\"->\"5\" \n  \"3\"->\"7\" \n  \"5\"->\"9\" \n  \"7\"->\"11\" \n  \"4\"->\"13\" \n  \"6\"->\"15\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
]

---

# Further built-in interpretations

.pull-left[
* Built-in .KULbginline[feature importance]:
  + `xgb.importance`: calculates .hi-pink[data]
  + `xgb.ggplot.importance`: .hi-pink[visual] representation


```r
*xgb.ggplot.importance(
* importance_matrix = xgb.importance(
    feature_names = colnames(mtpl_xgb),
    model = fit
  )
)
```

* Packages such as {vip} and {pdp} can also be used on `xgboost` models
  + even a [vignette](https://bgreenwell.github.io/pdp/articles/pdp-example-xgboost.html) dedicated to this
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-180-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Cross-validation with XGBoost

.KULbginline[Built-in cross-validation] with `xgb.cv`
  + same interface as the `xgboost` function
  + add `nfolds` to define the .hi-pink[number of folds]
  + add `stratified` for .hi-pink[stratification]


```r
set.seed(86493) # reproducibility
xval &lt;- xgb.cv(data = mtpl_xgb,
               nrounds = 200,
               early_stopping_rounds = 20,
               verbose = FALSE,
*              nfold = 5,
*              stratified = TRUE,
               params = list(booster = 'gbtree',
                             objective  = 'count:poisson',
                             eval_metric = 'poisson-nloglik',
                             eta = 0.1, nthread = 1,
                             subsample = 0.75, colsample_bynode = 0.5,
                             max_depth = 3, min_child_weight = 1000,
                             gamma = 0, lambda = 1, alpha = 1))

```

---

# Cross-validation results

Get the cross-validation .KULbginline[results] via `$evaluation_log`:


```r
xval$evaluation_log %&gt;% print(digits = 5)
##      iter train_poisson_nloglik_mean train_poisson_nloglik_std
##   1:    1                    0.88048                0.00017649
##   2:    2                    0.84983                0.00016304
##   3:    3                    0.82167                0.00020066
##   4:    4                    0.79477                0.00028473
##   5:    5                    0.76935                0.00019933
##  ---                                                          
## 196:  196                    0.38157                0.00097562
## 197:  197                    0.38157                0.00097635
## 198:  198                    0.38157                0.00097577
## 199:  199                    0.38156                0.00097688
## 200:  200                    0.38156                0.00097555
##      test_poisson_nloglik_mean test_poisson_nloglik_std
##   1:                   0.88052               0.00071933
##   2:                   0.85029               0.00083193
##   3:                   0.82171               0.00083297
##   4:                   0.79471               0.00083522
##   5:                   0.76925               0.00086406
##  ---                                                   
## 196:                   0.38251               0.00381020
## 197:                   0.38251               0.00381121
## 198:                   0.38251               0.00381100
## 199:                   0.38251               0.00380919
## 200:                   0.38250               0.00381351
```

---

# Cross-validation results



.pull-left[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-184-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="ML_part2_files/figure-html/unnamed-chunk-185-1.png" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
.KULbginline[That's a wrap] on .hi-pink[tree-based ML]! Now it's your time to experiment. &lt;br&gt;
Below are some .hi-pink[suggestions], but feel free to .KULbginline[get creative].

1. Perform a .hi-pink[tuning] exercise for your favorite tree-based algorithm. Beware that tuning can take up a lot of time, so do not overdo this.

1. Apply your favorite algorithm on a classification problem, for example to predict the .hi-pink[occurence] of a claim.

1. Use a .hi-pink[gamma] deviance to build a .KULbginline[severity] XGBoost model. The `mtpl` data contains the average claim amount in the feature `average`. Remember: if you want to develop a GBM with a gamma loss, you need the implementation available at Harry Southworth's [Github](https://github.com/harrysouthworth/gbm).

1. Develop a boosting or random forest model for the .hi-pink[Ames Housing] data and extract .KULbginline[insights] in the form of feature importance and partial dependence plots.

1. Compare the performance of a regression tree, random forest and boosting model. Which model performs .hi-pink[best]?


]

---

name: wrap-up

# Thanks!  &lt;img src="img/xaringan.png" class="title-hex"&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
&lt;br&gt; &lt;br&gt; &lt;br&gt;
Course material available via 
&lt;br&gt;
&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; https://github.com/katrienantonio/hands-on-machine-learning-R-module-2
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"highlightLines": true,
"countIncrementalSlides": false,
"highlightSpans": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
